#+TITLE: Visión por Computadora 
#+author: Eduardo Alcaraz
#+email: eduardo.ac@morelia.tecnm.mx

* ¿Que es la visión por computadora?
[[https://www.dropbox.com/scl/fo/54jqpwdzfnm6l0mdc6hur/h?rlkey=i85skh89ll342evwedk0t3x7r&st=kxmgh8vh&dl=0][Libros]]


La visión por computadora es un campo de la inteligencia artificial
asociado al análisis de imágenes y vídeos, que incluye un conjunto de
técnicas que otorgan a la computadora la capacidad de *ver* y extraer
información de aquello que se ha visto.

Los sistemas se componen de una cámara fotográfica o de vídeo y un
software especializado que identifica y clasifica objetos. Son capaces
de analizar imágenes (fotos, imágenes, vídeos, códigos de barras), así

* 
como caras y emociones.

Para enseñar a una computadora a *ver* , se utilizan tecnologías de
aprendizaje automático y se recopilan muchos datos que permiten
resaltar características y combinaciones de las mismas para
identificar aún más objetos similares.


* Visión por computadora
#startup:nlineimages
#+ATTR_LATEX: :width 0.2\textwidth
[[file:img/im1.jpg]]

* Aspectos Claves de la visión por computadora 

 - *Procesamiento de Imágenes*: Involucra técnicas para mejorar
   imágenes digitales, extraer información útil, realizar ajustes como
   corrección de color, reducción de ruido, y más.

 - *Reconocimiento de Patrones*: Se refiere a la identificación de
   patrones, formas y características en las imágenes. Esto puede
   incluir el reconocimiento de rostros, objetos, escenas, gestos,
   etc.

 - *Segmentación de Imágenes*: Se trata de dividir una imagen en
   diferentes partes o segmentos, a menudo para aislar regiones o
   elementos de interés.

 - *Detección y Seguimiento de Objetos*: Implica identificar y seguir
   objetos a lo largo del tiempo en una serie de imágenes o vídeos.

 - *Reconstrucción 3D*: Consiste en crear representaciones
   tridimensionales de un objeto o escena a partir de imágenes
   bidimensionales.

 - *Visión Artificial en Robótica*: Aplicación de la visión por
   computadora en robots para la navegación, manipulación de objetos y
   otras tareas.

 - *Interpretación de Escenas*: Comprende el análisis de escenas
   completas en términos de identificación de objetos, su disposición
   espacial, interacciones, y el contexto general.

La visión por computadora se utiliza en una variedad de aplicaciones,
que van desde la seguridad y vigilancia hasta la medicina, pasando por
los vehículos autónomos, la inspección industrial, la gestión de
contenido digital y la interacción hombre-máquina. Este campo se
beneficia enormemente de los avances en el aprendizaje automático y el
aprendizaje profundo, permitiendo el desarrollo de sistemas más
precisos y eficientes para el análisis visual.

* Libros de Visión por Computadora 
- Geometría de Vistas Múltiples en Visión por Computadora *Richard
  Hartley*
- Visión por Computadora Modelos, Aprendizaje e Inferencia *Simon
  J. D. Prince*
- Visión por Computadora Un Enfoque Moderno *David A. Forsyth*
- Aprendizaje Profundo Práctico para la Nube, Móvil y Edge Proyectos
  Reales de IA y Visión por Computadora Usando Python, Keras y
  TensorFlow *Anirudh Koul*
- Aprendiendo OpenCV 4 Visión por Computadora con Python 3 Entiende
  las herramientas, técnicas y algoritmos para la visión por
  computadora y el aprendizaje automático *Joseph Howse*

* Tecnologías y Herramientas
  - Lenguajes de Programación: Python, C++, C.
  - Bibliotecas y Frameworks: OpenCV, TensorFlow, PyTorch.

**   Instalación Python Opencv


*** Instalación python Opencv Windows


- *Paso 1: Instalar Python 3*
   - Python se puede descargar desde la página oficial: [[https://www.python.org/downloads/][Python Downloads]].
   - Asegúrate de marcar la opción "Add Python 3.x to PATH" durante la instalación.

- *Paso 2: Verificar la Instalación de Python*
   - Abre la Terminal de Comandos (Command Prompt) y ejecuta:
     #+BEGIN_SRC bash
     python --version
     #+END_SRC
   - Esto debería mostrar la versión de Python instalada.

- *Paso 3: Actualizar pip (Gestor de Paquetes de Python)*
   - En la Terminal de Comandos, ejecuta:
     #+BEGIN_SRC bash
     python -m pip install --upgrade pip
     #+END_SRC

- *Paso 4: Instalar OpenCV*
   - Utiliza pip para instalar OpenCV. En la Terminal de Comandos, ejecuta:
     #+BEGIN_SRC bash
     pip install opencv-python
     #+END_SRC
   - Si necesitas las funcionalidades adicionales de OpenCV, instala también opencv-contrib-python:
     #+BEGIN_SRC bash
     pip install opencv-contrib-python
     #+END_SRC

- *Paso 5: Verificar la Instalación de OpenCV*
   - Para verificar que OpenCV está instalado, abre un intérprete de Python y ejecuta:
     #+BEGIN_SRC python
     import cv2
     print(cv2.__version__)
     #+END_SRC
   - Si se muestra la versión de OpenCV sin errores, la instalación
     fue exitosa.

- *Notas Finales*
   - Es recomendable reiniciar el sistema después de instalar Python
     para asegurar que todos los cambios de configuración se apliquen
     correctamente.
   - Puede ser útil trabajar en un entorno virtual para proyectos de
     Python para gestionar las dependencias de manera más eficiente.

	


*** Instalación python Opencv Mac 

Instalar Python 3 y OpenCV en macOS es un proceso bastante sencillo. A
continuación, se muestran los pasos para realizar esta instalación.

- *Paso 1: Instalar Python 3*
macOS viene con Python 2.7 instalado por defecto, pero se recomienda usar Python 3 para proyectos nuevos.

   #+BEGIN_SRC bash
   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
   brew install python3
   #+END_SRC
   Verificar la versión de Python:
   #+BEGIN_SRC bash
   python3 --version
   #+END_SRC

- *Paso 2: Instalar pip*
   pip es el gestor de paquetes de Python y generalmente viene instalado con Python 3.
   Verificar si pip está instalado:
   #+BEGIN_SRC bash
   pip3 --version
   #+END_SRC
   Si pip no está instalado:
   #+BEGIN_SRC bash
   sudo easy_install pip
   #+END_SRC

- *Paso 3: Instalar OpenCV*
   Puedes instalar OpenCV para Python utilizando pip.
   Instalación básica:
   #+BEGIN_SRC bash
   pip3 install opencv-python
   #+END_SRC
   Instalar con funcionalidades adicionales:
   #+BEGIN_SRC bash
   pip3 install opencv-contrib-python
   #+END_SRC

- *Paso 4: Verificar la Instalación de OpenCV*
   Para comprobar que OpenCV esté correctamente instalado:
   #+BEGIN_SRC python
   import cv2
   print(cv2.__version__)
   #+END_SRC

- *Notas Finales*
  - Es una buena práctica trabajar en un entorno virtual para proyectos de Python.
  - Asegúrate de que tu sistema macOS esté actualizado.





*** Instalación python Opencv Linux

Instalar Python 3 y OpenCV en un sistema Linux generalmente es un
proceso sencillo. A continuación, te presento los pasos genéricos para
la mayoría de las distribuciones de Linux. Ten en cuenta que estos
pasos pueden variar ligeramente dependiendo de la distribución
específica que estés utilizando (como Ubuntu, Fedora, etc.).

- *Instalar Python 3*

La mayoría de las distribuciones modernas de Linux ya vienen con
Python 3 instalado. Puedes verificar si Python 3 está instalado y su
versión usando el siguiente comando en la terminal:

#+BEGIN_SRC shell
python3 --version
#+END_SRC

Si Python 3 no está instalado o deseas una versión más reciente,
puedes instalarlo a través del gestor de paquetes de tu distribución:

- *En distribuciones basadas en Debian (como Ubuntu):*

  #+BEGIN_SRC shell
sudo apt update
sudo apt install python3
#+END_SRC

- *En distribuciones basadas en Red Hat (como Fedora):*

#+BEGIN_SRC shell
 sudo dnf install python3
#+END_SRC

- *Instalar pip (Gestor de Paquetes de Python)*

*pip* es el gestor de paquetes para Python y se utiliza para instalar paquetes de Python como OpenCV. Puedes instalar `pip` con el siguiente comando:

- *En Ubuntu y otras distribuciones basadas en Debian:*

#+BEGIN_SRC bash
 sudo apt install python3-pip
#+END_SRC
 
- En Fedora y distribuciones basadas en Red Hat:

  sudo dnf install python3-pip


- *Instalar OpenCV*

Una vez que tengas Python 3 y pip instalados, puedes instalar OpenCV. El paquete `opencv-python` proporciona enlaces a las bibliotecas de OpenCV y es el método más fácil de instalar OpenCV para Python. Ejecuta el siguiente comando:

#+BEGIN_SRC bash
pip3 install opencv-python
#+END_SRC

Si necesitas los módulos adicionales (que incluyen algoritmos patentados), puedes instalar `opencv-contrib-python`:

#+BEGIN_SRC bash
pip3 install opencv-contrib-python
#+END_SRC

- *Verificar la Instalación*

Para verificar que OpenCV está correctamente instalado, puedes hacer lo siguiente:

1. Abre una terminal y escribe `python3` para entrar en el intérprete interactivo de Python.

2. En el intérprete, escribe:

#+BEGIN_SRC python
 import cv2
 print(cv2.__version__)
  
#+END_SRC
 
   Si no hay errores y se muestra la versión de OpenCV, significa que la instalación fue exitosa.

- *Notas Adicionales*

   - Si estás trabajando en un entorno de desarrollo profesional o experimental, es una buena práctica usar entornos virtuales para gestionar las dependencias de Python. Puedes usar herramientas como `venv` o `conda` para crear entornos virtuales.

   - Asegúrate de que tu sistema esté actualizado antes de comenzar la instalación.

   - Los pasos exactos pueden variar ligeramente dependiendo de la versión y el tipo de tu distribución de Linux. 


* Aplicaciones de la Visión por Computadora
  - Reconocimiento Facial: Uso en seguridad y dispositivos móviles.
  - Vehículos Autónomos: Navegación y detección de obstáculos.
  - Análisis Médico de Imágenes: Aplicación en diagnóstico y análisis.

** Programación 
*** Cargar imagen 
   #+BEGIN_SRC python :results output
import cv2 as cv 
img = cv.imread('/home/likcos/Imágenes/tr.png', 0)
cv.imshow('ejemplo', img)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC
   #+RESULTS:
*** Modelos de Color

   #+BEGIN_SRC python :results output
import cv2 as cv 
img = cv.imread('/home/likcos/Imágenes/tr.png', 1)
imgGris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)        
cv.imshow('ejemplo', img)
cv.imshow('ejemploGris', imgGris)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

#+RESULTS:

*** Filtros de convolución 
#+BEGIN_SRC python
import cv2 as cv 
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
mtz = np.array([[-1,-2,-1],
                [0,0,0],
                [1,2,1]])
resultado = cv.filter2D(img, -1, mtz)
cv.imshow('marcoc', resultado)
cv.imshow('marco', img)
cv.waitKey(0)
cv.destroyAllWindows()


#+END_SRC

#+RESULTS:
: None




*** Canales de color
#+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np 
img = cv.imread('img/tr.png')
img2 = np.zeros(img.shape[:2], dtype=np.uint8)
print(img.shape[1])
b,g,r =cv.split(img)
rb=cv.merge([b,img2,img2])
rg=cv.merge([img2,g,img2])
rr=cv.merge([img2,img2,r])

cv.imshow('img', img)
#cv.imshow('img2',img2)
cv.imshow('b',rb)
cv.imshow('g',rg)
cv.imshow('r',rr)
cv.waitKey(0)
cv.destroyAllWindows()


#+END_SRC

#+RESULTS:
: 635

*** Segmentación de color 

#+BEGIN_SRC python
import cv2 as cv
img = cv.imread('img/tr.png',1)
imghsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
imgRGB = cv.cvtColor(img, cv.COLOR_BGR2RGB)
ubb=(0,100, 100)
uba=(20, 255,255)
mask = cv.inRange(imghsv, ubb, uba)
res = cv.bitwise_and(img, img, mask=mask)
xcv.imshow('img', img)
cv.imshow('Resultado',res )
cv.imshow('mask', mask)
cv.waitKey(0)
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: None

*** Transformaciones Geométricas 

**** Transformaciones Afín 
  La transformación afín es una transformación geométrica que esta
  constituida por translación, escalamiento, rotación y
  cizallamiento. Cada una de estas Transformaciones es una
  Transformación afín. 
  
**** Traslación 
 Una translación la podemos hacer simplemente asumiendo que nuevas
 coordenadas $\hat{x} = x + t_x  \hat{y} = y + t_y$ les sumamos un valor
 $t_x$ p $t_y$ según corresponda. En coordenadas homogéneas queda como 

$$
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&\\
 1
 \end{pmatrix}
 = 
 \begin{pmatrix}
 1 & 0& t_x\\
 0 & 1 & t_y&\\
 0 & 0 & 1
 \end{pmatrix}
\begin{pmatrix}
 x\\
 y&\\
 1
 \end{pmatrix} 
$$



 #+BEGIN_SRC python
import cv2 as cv
import numpy as np
img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*2, w*2, 1) , dtype = "uint8")
print("Valores " + str(img.shape[:2]))
for i in range(h):
    for j in range(w):
        img2[int(i*0.5),int(j*0.5)]=img[i,j]

cv.imshow('imagen', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

 #+END_SRC
  

 #+RESULTS:
 : None


**** Escalamiento 
	 El escalamiento puede entenderse como hacer una figura geométrica
	 cambie su tamaño o cambie su escala. Un escalamiento en x lo
	 podemos representar por como $\hat{x} = x$, $s_x$ y en y como
	 $\hat{y} = y$,  $s_y$ En coordenada homogéneas se puede expresar como 
$$
 \begin{pmatrix}
 \hat{xs}\\
 \hat{y}&\\
 1
 \end{pmatrix}
 = 
 \begin{pmatrix}
 s_x & 0& 0\\
 0 & s_y & 0&\\
 0 & 0 & 1
 \end{pmatrix}
 
\begin{pmatrix}
 x\\
 y&\\
 1
 \end{pmatrix}
$$	

 #+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np
img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
print(h, w)
img2 = np.zeros((h*2, w*2) , dtype = "uint8")
print("Valores " + str(img.shape[:2]))
for i in range(h):
    for j in range(w):
        img2[int(i*2),int(j*2)]=img[i,j]

cv.imshow('imagen', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

 #+END_SRC

 #+RESULTS:
 : 441 524
 : Valores (441, 524)

**** Rotación 

 Considerando el caso de un punto que rota respecto a un punto
 fijo. Las coordenadas x y y, en forma polar las podemos obtener como $x=r$ 
 $cos(\theta) y y = r sen(\theta)$. Si consideramos que esta gira un ángulo $\theta$    
 entonces podemos representar esta rotación en forma polar. 
 
$$
 \begin{equation}
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
 r cos(\alpha + \theta)\\
 r sen(\alpha + \theta)
 \end{pmatrix}
 = 
 \begin{pmatrix}
 r cos(\alpha + \theta) - r sin(\alpha) sin(\theta) \\
 r sen(\alpha + \theta) + r sin(\alpha) con(\theta)
 \end{pmatrix}
 \end{equation}


 \begin{equation}
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
 x cos(\theta) - y sin(\theta) \\
 x sen(\theta) + y cos(\theta)
 \end{pmatrix}
 \end{equation}


 \begin{equation}
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
  cos(\theta) &-  sin(\theta) \\
  sen(\theta) &  cos(\theta)
 \end{pmatrix}
 \begin{pmatrix}
 x \\
 y 
 \end{pmatrix}

 \end{equation}

 \begin{equation}
 xcos(\theta) - ysin(\theta), xsen(\theta) + ycos(\theta)
 \end{equation}
$$
 
#+BEGIN_SRC python :results output
import cv2 as cv
import math
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*3, w*3), dtype = "uint8")
for i in range(h):
    for j in range(w):
        img2[int(i*math.cos(math.radians(30))-j*math.sin(math.radians(30)))+200,
             int(i*(math.sin(math.radians(30)))+j*math.cos(math.radians(30)))+50]=img[i,j]
cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()
 #+END_SRC

 #+RESULTS:

**** Cizallamiento 

   El cizallamiento es una transformación dada por la matriz, donde $c_x$
   es el ángulo de cizallamiento respecto al eje x

   \begin{equation}
   C_x
   = 
   \begin{pmatrix}
   1 & tg(C_x)& 0\\
   0 & 1 & 0&\\
   0 & 0 & 1
   \end{pmatrix}

   \end{equation}



   #+BEGIN_SRC python :results output
import cv2 as cv
import math
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*2, w*2), dtype = "uint8")
matz = np.array([[1,1,1],[1,1,1],[1,1,1]])
for i in range(h):
    for j in range(w):
        img2[int(i*2) ,int(j*2)]=img[i,j]
res = cv.filter2D(img2, -1, matz)
cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.imshow('imagen3', res)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

   #+RESULTS:

*** Traslación Opencv  WarpAffine Afine

   #+BEGIN_SRC python
import cv2 as cv
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png')
h,w = img.shape[:2]
mw = np.float32([[1,0,10],[0,1,10]])
img2 = cv.warpAffine(img,mw,(h,w))

cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()


   #+END_SRC

   #+RESULTS:
   : None

**** Rotación Opencv WarpAffine + getRotationMatrix2D

   #+BEGIN_SRC python
import cv2 as cv
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/mo1.png')
h,w = img.shape[:2]

mw = cv.getRotationMatrix2D((h//2, w//2),30,-1)
img2 = cv.warpAffine(img,mw,(h,w))

cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

   #+RESULTS:
   : None

*** Primitivas de Dibujo

   #+BEGIN_SRC python
import cv2 as cv 
import numpy as np 
img = 58*np.ones((1000,1000,3), np.uint8)
cv.line(img,(0,0), (100,100), (23, 189, 200), 3)
cv.rectangle(img, (40,40), (80,80), (1,65,90), -1)
cv.circle(img, (100,100), 50, (45, 190,200),-1)
cv.circle(img, (100,100), 45, (45, 200,90),-1)
cv.ellipse(img,(256,256),(100,50),0,0,180,255,-1)
pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)
pts = pts.reshape((-1,1,2))
cv.polylines(img,[pts],True,(0,255,255))
cv.imshow('marco',img)
cv.waitKey(0)
cv.destroyAllWindows()

   #+END_SRC

   #+RESULTS:


   #+begin_src python :results output
import cv2 as cv 
import numpy as np 
import math

Pi = 3.1416
img = 255 * np.ones((500, 500, 3 ), np.uint8)

for i in range(360):
    #img = 255 * np.ones((500, 500, 3 ), np.uint8)
    h, w = img.shape[:2] 
   
    #x = int(h/2) + int(100* math.sin(6*(i*(Pi/180))))*math.sin(i*Pi/180)
    #y = int(w/2) + int(100* math.sin(6*(i*(Pi/180))))*math.cos(i*Pi/180)
    
    #xx = int(h/3) + int(100* (-1+math.cos(i*(Pi/180)))*math.sin(i*Pi/180))
    #yy = int(w/3) + int(100* (-1+math.cos(i*(Pi/180)))*math.cos(i*Pi/180))

    xx = int(h/2) + int(100* (math.cos(1*(i*(Pi/180))))*(-1*(math.cos(80*(i*Pi/180)))))
    yy = int(w/2) + int(100* (math.sin(1*(i*(Pi/180))))*(-1*(math.sin(80*(i*Pi/180)))))

    #cv.circle(img, (int(x) , int(y)), 3, (0,i,0), -1)
    #cv.circle(img, (int(y) , int(x)), 3, (i,0,0), -1)
    cv.circle(img, (int(xx) , int(yy)), 1, (0,0,i), -1)
    #cv.imwrite('resultado'+str(i)+'.jpg',img)

    cv.imshow('imagen', img)
    cv.waitKey(10)

cv.imshow('imagen', img)
cv.imwrite('resultado.jpg',img)
cv.waitKey(0)
cv.destroyAllWindows()
   
   #+end_src



*** Flujo óptico 

El flujo óptico es un concepto en visión por computadora y
procesamiento de imágenes que se refiere al patrón de movimiento
aparente de los objetos, las superficies y los bordes en una escena
visual causado por el movimiento relativo entre un observador y la
escena. La idea es estimar cómo se mueven los puntos de una imagen
entre dos cuadros consecutivos de un video o entre dos imágenes
tomadas en momentos diferentes.

*Conceptos Clave del Flujo Óptico:* Vector de Movimiento: Cada punto en
la imagen tiene asociado un vector que indica la dirección y la
magnitud del movimiento de ese punto entre dos cuadros.

*Consistencia de Brillo*: Se asume que el brillo (intensidad) de un
punto en la imagen permanece constante entre cuadros consecutivos, lo
que permite relacionar los puntos en diferentes cuadros.

*Restricciones Espaciales y Temporales*: Se considera que los puntos
vecinos en una imagen tienden a tener movimientos similares, y este
movimiento cambia suavemente a lo largo del tiempo.

*Métodos para Calcular el Flujo Óptico*: Métodos Basados en Gradientes:
Utilizan las variaciones del brillo y los gradientes de la imagen para
calcular el movimiento. Un ejemplo es el algoritmo de Lucas-Kanade,
que asume que el flujo óptico es esencialmente constante en una
pequeña ventana de la imagen.

*Métodos Basados en Bloques*: Comparan bloques (pequeñas áreas) de un
cuadro con los del cuadro siguiente, buscando el bloque que mejor se
ajuste. Esto se hace por ejemplo en la técnica de coincidencia de
bloques.

*Métodos Basados en Características*: Identifican características
distintivas en las imágenes (como esquinas o bordes) y rastrean cómo
se mueven estas características entre los cuadros.

*Métodos Basados en Aprendizaje Profundo*: Utilizan redes neuronales
para aprender y predecir el movimiento en secuencias de imágenes.

*Aplicaciones del Flujo Óptico*:
*Seguimiento de Objetos*: Rastrear el movimiento de objetos en videos.
*Estabilización de Video*: Corregir la sacudida en las grabaciones de video.
*Reconstrucción de Escenas 3D*: Ayuda a entender la estructura tridimensional del entorno.
*Análisis de Movimiento*: En deportes o medicina para analizar movimientos del cuerpo humano.

*Limitaciones:* 
Sensible a cambios de iluminación.  No funciona bien en
escenas con mucho movimiento o sin texturas.  
La asunción de consistencia de brillo no siempre es válida.  
El flujo óptico es una herramienta poderosa en visión por computadora, pero su precisión y
eficacia dependen en gran medida del método específico utilizado y de
las características de la escena que se está analizando.

#+BEGIN_SRC python :results output
import numpy as np 
import cv2 as cv

cap = cv.VideoCapture(0)


lkparm =dict(winSize=(15,15), maxLevel=2,
             criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03)) 


_, vframe = cap.read()
vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
p0 = np.array([(100,100), (200,100), (300,100), (400,100), (500,100),
               (100,200), (200,200), (300,200), (400,200), (500,200),
               (100,300), (200,300), (300,300), (400,300), (500,300),
               (100,400), (200,400), (300,400), (400,400), (500,400)])

p0 = np.float32(p0[:, np.newaxis, :])

mask = np.zeros_like(vframe)
cad =''

while True:
    _, frame = cap.read()
    fgris = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    p1, st, err = cv.calcOpticalFlowPyrLK(vgris, fgris, p0, None, **lkparm) 

    if p1 is None:
        vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
        p0 = np.array([(100,100), (200,100), (300,100), (400,100) ])
        p0 = np.float32(p0[:, np.newaxis, :])
        mask = np.zeros_like(vframe)
        cv.imshow('ventana', frame)
    else:
        bp1 = p1[st ==1]
        bp0 = p0[st ==1]
        
        for i, (nv, vj) in enumerate(zip(bp1, bp0)):
            a, b = (int(x) for x in nv.ravel())
            c, d = (int(x) for x in vj.ravel())
            dist = np.linalg.norm(nv.ravel() - vj.ravel())

            #print(i, dist)
            
            
            
            frame = cv.line(frame, (c,d), (a,b), (0,0,255), 2)
            frame = cv.circle(frame, (c,d), 2, (255,0,0),-1)
            frame = cv.circle(frame, (a,b), 3, (0,255,0),-1)
        cv.imshow('ventana', frame)

        vgris = fgris.copy()

        if(cv.waitKey(1) & 0xff) == 27:
            break

cap.release()
cv.destroyAllWindows()
#+END_SRC

#+BEGIN_SRC python :results output
import numpy as np 
import cv2 as cv

cap = cv.VideoCapture(0)


lkparm =dict(winSize=(15,15), maxLevel=2,
             criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03)) 


_, vframe = cap.read()
vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
p0 = np.array([(100,100), (200,100), (300,100), (400,100)])
p0 = np.float32(p0[:, np.newaxis, :])

mask = np.zeros_like(vframe)
cad =''

while True:
    _, frame = cap.read()
    fgris = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    p1, st, err = cv.calcOpticalFlowPyrLK(vgris, fgris, p0, None, **lkparm) 

    if p1 is None:
        vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
        p0 = np.array([(100,100), (200,100), (300,100), (400,100) ])
        p0 = np.float32(p0[:, np.newaxis, :])
        mask = np.zeros_like(vframe)
        cv.imshow('ventana', frame)
    else:
        bp1 = p1[st ==1]
        bp0 = p0[st ==1]
        
        for i, (nv, vj) in enumerate(zip(bp1, bp0)):
            a, b = (int(x) for x in nv.ravel())
            c, d = (int(x) for x in vj.ravel())
            dist = np.linalg.norm(nv.ravel() - vj.ravel())

            #print(i, dist)
            
            if i == 0 and dist > 30 :
                cad = cad + '0' 
            elif i == 1 and dist > 30 :
                cad = cad + '1' 
            elif i == 2 and dist > 30 :    
                print('2', dist)
                cad = cad + '2' 
            elif i== 3 and dist > 30 :
                cad= cad+'3' 
           
            frame = cv.putText(frame, cad, (50,50),
                               cv.FONT_HERSHEY_SIMPLEX, 1 , (255,0,0),2, cv.LINE_AA)    
            frame = cv.putText(frame, str(i), (c,d),
                               cv.FONT_HERSHEY_SIMPLEX, 1 , (255,0,0),2, cv.LINE_AA)    
            
            frame = cv.line(frame, (c,d), (a,b), (0,0,255), 2)
            frame = cv.circle(frame, (c,d), 2, (255,0,0),-1)
            frame = cv.circle(frame, (a,b), 3, (0,255,0),-1)
        cv.imshow('ventana', frame)

        vgris = fgris.copy()

        if(cv.waitKey(1) & 0xff) == 27:
            break

cap.release
cv.destroyAllWindows()
  #+END_SRC


#+BEGIN_SRC python :results output
import numpy as np
import cv2 as cv

# Inicializa la captura de video desde la cámara
cap = cv.VideoCapture(0)

# Parámetros para el flujo óptico de Lucas-Kanade
lk_params = dict(winSize=(15, 15), maxLevel=2,
                 criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))

# Captura el primer frame y conviértelo a escala de grises
ret, old_frame = cap.read()
old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)

# Define el punto inicial para el seguimiento (en este caso, el centro del rectángulo)
start_point = np.array([[old_frame.shape[1] // 2, old_frame.shape[0] // 2]], np.float32)


while True:
    # Captura un nuevo frame
    ret, frame = cap.read()
    if not ret:
        break
    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)

    # Calcula el flujo óptico
    new_points, status, error = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, start_point, None, **lk_params)

    # Selecciona los puntos buenos
    if status[0] == 1:
        good_new = new_points[0]
        good_old = start_point[0]

        # Dibuja el circulo en la nueva posición
        a, b = good_new.ravel()
        start_point = np.array([[a, b]], np.float32)
        center = (int(a), int(b))
        frame = cv.circle(frame, center, 50, (0, 255, 0), -1)

    # Muestra el frame con el rectángulo
    cv.imshow('Flujo Optico', frame)

    # Actualiza el frame anterior y los puntos
    old_gray = frame_gray.copy()

    # Salir del bucle si se presiona la tecla 'Esc'
    if cv.waitKey(1) & 0xFF == 27:
        break

# Libera los recursos
cap.release()
cv.destroyAllWindows()
#+END_SRC


  
#+RESULTS:
#+begin_example
2 120.083694
2 120.879524
2 140.26334
2 186.67227
2 137.3906
2 62.329464
2 64.929184
2 69.902626
2 54.96874
2 52.69248
2 60.124416
2 63.391445
2 89.591415
2 56.381046
2 62.464
2 43.80231
2 39.35062
2 49.12454
2 46.783375
2 38.045883
2 33.4635
2 31.740929
2 57.095345
2 41.95649
2 82.12661
2 58.450493
2 50.61094
2 56.976192
2 52.000736
2 87.50451
2 53.20077
2 67.31267
2 69.54097
2 63.143032
2 35.948177
2 95.95501
2 74.23872
2 76.64836
2 68.564
2 87.3111
2 74.414635
2 72.70697
2 61.554096
#+end_example


*** Vídeo

**** Cargar vídeo simple opencv 
  #+BEGIN_SRC python :results output

import cv2 as cv 

cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        cv.imshow('video', img)
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()
  #+END_SRC

 #+RESULTS:

**** División de canales de color en vídeo
 #+BEGIN_SRC python
import cv2 as cv 
import numpy as np
cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        img2 = np.zeros(img.shape[:2], dtype=np.uint8)
        b,g,r =cv.split(img)
        rb=cv.merge([g,r,b])
        rg=cv.merge([r,g,b])
        rr=cv.merge([b,r,r])
        #imgGris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)        
        cv.imshow('b',rb)
        cv.imshow('g',rg)
        cv.imshow('r',rr)
        cv.imshow('video', img)
        #cv.imshow('videogris', imgGris)
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()
 #+END_SRC

**** Seguimiento por color 

#+BEGIN_SRC python
import cv2 as cv 

cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        #cv.imshow('video', img)
        imghsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        ubb=(100,100, 100)
        uba=(130, 255,255)
        mask = cv.inRange(imghsv, ubb, uba)
        res = cv.bitwise_and(img, img, mask=mask)
        cv.imshow('resultado', res)
        cv.imshow('hsv', imghsv)
        cv.imshow('mask', mask)
        
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()




#+END_SRC

#+RESULTS:
: None




*** Haarcascades 
Los Haar Cascades son una técnica utilizada en el campo de la visión
por computadora para la detección de objetos. Fueron introducidos por
Paul Viola y Michael Jones en su artículo seminal "Rapid Object
Detection using a Boosted Cascade of Simple Features" en 2001. Esta
técnica es particularmente conocida por su eficacia en la detección de
rostros, aunque puede ser utilizada para detectar otros tipos de
objetos.

#+startup: inlineimages
#+ATTR_LATEX: :width 0.3\textwidth
[[file:img/cascade.png]]

**** Conceptos Clave: 
Características de Haar: Son patrones visuales
 simples que se pueden calcular rápidamente en una imagen. Estas
 características se asemejan a pequeñas versiones de núcleos de wavelet
 de Haar y son utilizadas para capturar la presencia de bordes, cambios
 de textura, y otras propiedades visuales.

 
**** Imágenes Integrales: 
Para acelerar el cálculo de las características
 de Haar, se utiliza un concepto llamado imagen integral. Una imagen
 integral permite calcular la suma de los valores de los píxeles en
 cualquier área rectangular de la imagen en tiempo constante.

****  Adaboost: 
Es un método de aprendizaje automático utilizado para
 mejorar la eficiencia de la detección. Selecciona un pequeño número
 de características críticas de un conjunto más grande y construye
 clasificadores "débiles". Luego, estos se combinan en un clasificador
 más fuerte y eficiente.

****  Cascadas: 
En lugar de aplicar todas las características a una ventana de la
imagen, se organizan en una secuencia de etapas (cascadas). Cada etapa
tiene su propio clasificador (hecho con Adaboost) y solo pasa las
ventanas de la imagen que parecen prometedoras. Esto reduce
significativamente el tiempo de cálculo, ya que muchas ventanas no
pasan las primeras etapas.

 *Proceso de Detección*: 
Pre-procesamiento: Se convierte la imagen en
 escala de grises y se crea su imagen integral.

 *Aplicación de las Características*: Se desplaza una ventana sobre la
 imagen, y en cada posición, se calculan las características de Haar.

 *Clasificación en Cascada*: Cada ventana es evaluada a través de la
 cascada de clasificadores. Si una ventana falla en alguna etapa, se
 descarta. Si pasa todas las etapas, se considera como una detección.

 *Post-procesamiento*: Finalmente, se pueden aplicar técnicas como la
 supresión de no máximos para reducir falsos positivos y mejorar la
 precisión.

 *Aplicaciones*: Detección de rostros en imágenes y videos.  Detección
 de peatones u otros objetos en sistemas de vigilancia.  Aplicaciones
 de realidad aumentada.  Es importante mencionar que, aunque los Haar
 Cascades fueron revolucionarios en su momento, han sido superados en
 precisión y velocidad por técnicas más modernas de aprendizaje
 profundo. Sin embargo, siguen siendo utilizados debido a su
 simplicidad y bajo requerimiento de recursos computacionales.

**** Ejemplo de un Haarcascade

https://github.com/opencv/opencv/tree/master/data/haarcascades

https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html

https://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html

https://amin-ahmadi.com/cascade-trainer-gui/
#+BEGIN_SRC python
import numpy as np
import cv2 as cv
import math 

rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)
i = 0  
while True:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gray, 1.3, 5)
    for(x, y, w, h) in rostros:
       #frame = cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)
       frame2 = frame[ y:y+h, x:x+w]
        #frame3 = frame[x+30:x+w-30, y+30:y+h-30]
       frame2 = cv.resize(frame2, (100, 100), interpolation=cv.INTER_AREA)
       cv.imwrite('/home/likcos/pruebacaras/juan/juan'+str(i)+'.jpg', frame2)
       cv.imshow('rostror', frame2)
    cv.imshow('rostros', frame)
    i = i+1
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
import cv2 as cv 

rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)

while True:
    ret, img = cap.read()
    gris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gris, 1.3, 5)
    for(x,y,w,h) in rostros:
        res = int((w+h)/8)
        img = cv.rectangle(img, (x,y), (x+w, y+h), (234, 23,23), 2)
        img = cv.rectangle(img, (x,int(y+h/2)), (x+w, y+h), (0,255,0),5 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 5, (0, 0, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 5, (0, 0, 255), -1 )

    cv.imshow('img', img)
    if cv.waitKey(1)== ord('q'):
        break
    
cap.release
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

* Reconocimiento de Personas

** Eigenfaces 

Un Eigenface (en español cara propia) es el nombre dado a un conjunto
de vectores propios cuando se utiliza en el problema de visión
artificial del reconocimiento de rostros humanos. Sirovich y Kirby
desarrollaron el enfoque de usar caras propias para el reconocimiento
y lo usaron Matthew Turk y Alex Pentland en la clasificación de
caras. Los vectores propios se derivan de la matriz de covarianza de
la distribución de probabilidad sobre el espacio vectorial de alta
dimensión de imágenes de rostros. Las caras propias forman un conjunto
base de todas las imágenes utilizadas para construir la matriz de
covarianza. Esto produce una reducción de la dimensión al permitir que
el conjunto más pequeño de imágenes base represente las imágenes de
entrenamiento originales. La clasificación se puede lograr comparando
cómo se representan las caras por el conjunto base.

 *Generación*
 Se puede generar un conjunto de caras propias mediante la realización
 de un proceso matemático llamado análisis de componentes principales
 (PCA) en un gran conjunto de imágenes que representan diferentes
 rostros humanos. De manera informal, las caras propias pueden
 considerarse un conjunto de "ingredientes faciales estandarizados",
 derivados del análisis estadístico de muchas imágenes de
 rostros. Cualquier rostro humano puede considerarse una combinación
 de estos rostros estándar. Por ejemplo, la cara de uno podría estar
 compuesta por la cara promedio más el 10 % de la cara propia 1, el 55
 % de la cara propia 2 e incluso el −3 % de la cara
 propia 3. Sorprendentemente, no se necesitan muchas caras propias
 combinadas para lograr una aproximación justa de la mayoría de las
 caras. Además, debido a que la cara de una persona no se registra
 mediante una fotografía digital, sino simplemente como una lista de
 valores (un valor para cada cara propia en la base de datos
 utilizada), se ocupa mucho menos espacio para la cara de cada
 persona.

 Las caras propias que se crean aparecerán como áreas claras y oscuras
 que se organizan en un patrón específico. Este patrón es cómo se
 seleccionan las diferentes características de una cara para
 evaluarlas y puntuarlas. Habrá un patrón para evaluar la simetría, si
 hay algún estilo de vello facial, dónde está la línea del cabello o
 una evaluación del tamaño de la nariz o la boca. Otras caras propias
 tienen patrones que son menos fáciles de identificar, y la imagen de
 la cara propia puede parecerse muy poco a una cara.

 La técnica utilizada en la creación de caras propias y su uso para el
 reconocimiento también se utiliza fuera del reconocimiento facial:
 reconocimiento de escritura a mano, lectura de labios, reconocimiento
 de voz, lenguaje de señas /interpretación de gestos con las manos y
 análisis de imágenes médicas. Por lo tanto, algunos no usan el
 término "eigenface", sino que prefieren usar 'eigenimage'.



#+BEGIN_SRC python :results output
import cv2 as cv 
import numpy as np 
import os
dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
print(np.count_nonzero(np.array(labels)==0)) 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloEigenface.xml')

#+END_SRC

#+RESULTS:
: ['lalo']
: 169

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.read('laloEigenface.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        #cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] > 2800:
            cv.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:



** Fisherfaces


 El algoritmo Fisherfaces es una técnica de reconocimiento facial que
 forma parte del campo del aprendizaje automático y la visión por
 computadora. Este algoritmo es una extensión del método de Análisis de
 Componentes Principales (PCA) y fue diseñado específicamente para
 mejorar la capacidad de reconocimiento en situaciones donde la
 iluminación y las expresiones faciales varían significativamente.

 La idea central detrás de Fisherfaces es reducir la dimensionalidad de
 las imágenes faciales manteniendo al mismo tiempo la capacidad de
 distinguir entre diferentes clases (es decir, diferentes
 personas). Esto se logra mediante el Análisis Discriminante Lineal
 (LDA), que es la base del método Fisherfaces. 

 Preprocesamiento: Las imágenes faciales se normalizan en términos de
 tamaño, orientación e iluminación.

 *Análisis de Componentes Principales (PCA)*: Se realiza PCA para reducir
 la dimensionalidad de los datos. PCA identifica las direcciones en las
 que los datos varían más y proyecta los datos en un espacio de menor
 dimensión preservando estas variaciones principales.

 *Análisis Discriminante Lineal (LDA)*: Después de aplicar PCA, se
 utiliza LDA para encontrar las combinaciones lineales de
 características que mejor separan las diferentes clases (diferentes
 personas). Mientras que PCA busca direcciones que maximizan la
 varianza en los datos, LDA busca maximizar la separación entre las
 diferentes clases.

 *Proyección y Clasificación*: Las imágenes se proyectan en el espacio de
 características obtenido por PCA y LDA. Luego, se utiliza un
 clasificador (como k-NN o máquinas de vectores de soporte) para
 identificar a qué clase (persona) pertenece cada imagen proyectada
 basándose en las características extraídas.

 El algoritmo Fisherfaces es particularmente efectivo en situaciones
 donde las variaciones entre las imágenes de una misma clase (por
 ejemplo, las diferentes expresiones faciales de una persona) son
 menores en comparación con las variaciones entre clases diferentes
 (diferentes personas). Esto lo hace robusto frente a cambios en la
 iluminación y las expresiones faciales, siendo una técnica popular en
 aplicaciones de reconocimiento facial.

#+CAPTION: Script para leer un dataset y generar el entrenamiento con FisherFaces
 #+BEGIN_SRC python ::results
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloFisherFace.xml')


 #+END_SRC

 #+RESULTS:
 : None

 #+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.read('laloFisherFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 500:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()



 #+END_SRC

 #+RESULTS:
 : None



** LBPH
El LBPH es un enfoque simple y efectivo para el reconocimiento
facial. A diferencia de otros métodos que operan en todo el rostro, el
LBPH trabaja examinando características locales. Su popularidad se
debe a su simplicidad, velocidad y buen rendimiento, incluso en
condiciones de iluminación desafiantes. Aquí está cómo funciona:

División de la Imagen en Celdas: La imagen del rostro se divide en
pequeñas regiones o celdas.

*Calculo de Patrones Binarios Locales (LBP):* Para cada píxel en una
región, se compara su intensidad con las de sus vecinos (generalmente
8 vecinos circundantes). Si la intensidad del vecino es mayor o igual
que el píxel central, se asigna un 1, de lo contrario un 0. Esto
genera un número binario de 8 dígitos (o un número decimal después de
la conversión) para cada píxel.

*Histogramas:* Se calcula un histograma de estas etiquetas LBP para cada
celda. Los histogramas cuentan la frecuencia de cada número obtenido
en el paso anterior dentro de la celda.

*Concatenación de Histogramas:* Los histogramas de todas las celdas se
concatenan en un solo vector de características. Este vector describe
las características locales de la imagen de la cara.

*Reconocimiento:* Para reconocer un rostro desconocido, se calcula su
vector de características LBPH y se compara con los vectores de
características de las caras conocidas (generalmente usando una medida
de distancia, como la distancia euclidiana). La imagen desconocida se
identifica como la clase (es decir, la persona) cuyo vector de
características conocido sea más cercano al del rostro desconocido.

El LBPH es eficaz en diversas condiciones y no requiere un
preprocesamiento tan intenso como otros métodos de reconocimiento
facial. Puede manejar variaciones en iluminación y expresión facial
bastante bien. Además, su implementación es relativamente sencilla, lo
que lo hace popular para aplicaciones en tiempo real y sistemas
embebidos.

#+BEGIN_SRC python
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloLBPHFace.xml')

#+END_SRC

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.read('laloLBPHFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 70:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2) 
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()


#+END_SRC


* Redes Neuronales Convolucionales 

** ¿Cómo funcionan las Convolutional Neural Networks?


La Red Neuronal Convolucional (CNN) es una forma avanzada de Red
Neuronal Artificial diseñada para el procesamiento de imágenes. Emula
la manera en que el cortex visual humano procesa la información
visual, lo que le permite identificar y clasificar distintas
características en las imágenes. Esta capacidad la convierte en una
herramienta eficaz para la "visión" computarizada y el reconocimiento
de objetos.

Las CNN están compuestas por múltiples capas ocultas, cada una
especializada y jerarquizada. Las capas iniciales suelen detectar
elementos simples como líneas y curvas. A medida que la información
avanza a través de la red, las capas subsiguientes procesan aspectos
más complejos, como formas específicas y patrones. Esta progresión
permite a las capas más profundas reconocer objetos más complejos,
como rostros o siluetas animales.

Para que una CNN aprenda a identificar una amplia gama de objetos en
imágenes, es crucial entrenarla con un extenso conjunto de datos. Por
ejemplo, para el reconocimiento de gatos, se necesitarían miles de
imágenes que muestren variaciones en color, raza, postura y
entorno. Este entrenamiento supervisado permite a la red aprender las
características distintivas de cada objeto y generalizar esta
comprensión para reconocer nuevas imágenes de manera efectiva. Así,
una CNN bien entrenada puede distinguir un gato independientemente de
su posición, color o tamaño, adaptándose a una amplia gama de
situaciones visuales.

** Convolución

La convolución es una operación matemática que combina dos conjuntos
de información. En el contexto de redes neuronales convolucionales
(CNN), la convolución se refiere a aplicar un filtro (también conocido
como kernel) a una imagen de entrada para producir un mapa de
características.  Cómo Funciona:

Un filtro de tamaño fijo recorre la imagen de entrada.  En cada
posición del filtro, se realiza una multiplicación punto a punto entre
los valores del filtro y los valores de la imagen bajo el filtro.  Los
productos resultantes se suman para obtener un único valor en el mapa
de características.  Este proceso se repite para cada posición del
filtro, generando un mapa de características que resalta ciertas
características de la imagen, como bordes, texturas y patrones.
Propósitos:

Extraer características locales de la imagen.  Capturar patrones
espaciales y relaciones en la imagen.  Reducir la dimensionalidad
espacial de la imagen mientras se mantiene la información relevante.


** Padding
Añadir píxeles adicionales alrededor de los bordes de la entrada antes
de la operación de convolución para controlar el tamaño de la salida y
conservar información de los bordes.

** Strides (Paso)

Número de píxeles que el filtro (kernel) se desplaza sobre la entrada
durante la operación de convolución. Un stride mayor que 1 reduce la
resolución espacial de la salida.

** Max Pooling Definición:

Operación que reduce la dimensionalidad espacial de la entrada tomando
el valor máximo dentro de una ventana específica, manteniendo las
características más importantes y reduciendo el tamaño de la salida.

** Stacking (Apilamiento)

Práctica de añadir múltiples capas convolucionales y de pooling en una
secuencia dentro de una red neuronal, permitiendo la construcción de
redes profundas y la extracción de características cada vez más
complejas.

#+startup: inlineimages
#+ATTR_LATEX: :width 0.3\textwidth
[[file:img/conv.png]]



** YOLO
YOLO (You Only Look Once) es un algoritmo de detección de objetos en
tiempo real que se ha destacado por su velocidad y precisión. Fue
desarrollado por Joseph Redmon y sus colaboradores. 

***  YOLO (You Only Look Once)

- *Enfoque Unificado*: A diferencia de otros métodos que aplican el
  proceso de detección en varias etapas, YOLO trata la detección de
  objetos como un problema de regresión único, convirtiendo la imagen
  de entrada directamente en coordenadas de cajas delimitadoras y
  probabilidades de clase en una sola pasada de la red neuronal.

- *Red Convolucional*: YOLO utiliza una red neuronal convolucional
  profunda para extraer características de la imagen.

- *División de la Imagen*: La imagen de entrada se divide en una
  cuadrícula \textit{times}. Cada celda de la cuadrícula es
  responsable de predecir un número fijo de cajas delimitadoras y las
  probabilidades de las clases para esas cajas.

***  Predicciones
- *Cajas Delimitadoras*: Cada celda de la cuadrícula predice un conjunto
  de cajas delimitadoras (bounding boxes), cada una con:
  - Coordenadas \( (x, y) \): La posición del centro de la caja con respecto a los bordes de la celda de la cuadrícula.
  - Ancho y alto \( (w, h) \): Dimensiones de la caja delimitadora relativas al tamaño de la imagen.
  - Confianza de la caja: Una puntuación que indica la probabilidad de que la caja contenga un objeto y la precisión de la caja delimitadora.
- *Probabilidades de Clase*: Cada celda también predice la probabilidad de las clases para los objetos presentes en las cajas delimitadoras.

***  Funcionamiento
- *Paso 1: Entrada*: Una imagen de entrada se pasa a través de la red
  convolucional.
- *Paso 2: División de Cuadrícula*: La imagen se divide en una
  cuadrícula \( S \times S \).
- *Paso 3: Predicciones de la Red*: La red predice \( B \) cajas
  delimitadoras y \( C \) probabilidades de clase para cada celda de
  la cuadrícula.
- *Paso 4: Filtrado de Cajas*: Se aplican técnicas como el umbral de
  confianza y la supresión de no-máximos (Non-Maximum Suppression,
  NMS) para filtrar las cajas delimitadoras y eliminar las
  redundantes.
- *Paso 5: Salida*: Las cajas delimitadoras finales con las
  probabilidades de clase correspondientes se devuelven como
  resultado.

*** Ventajas
- *Velocidad*: YOLO es extremadamente rápido porque procesa la imagen
  completa en una sola pasada de la red, lo que permite la detección
  en tiempo real.
- *Contexto Global*: Al considerar la imagen completa, YOLO puede
  capturar el contexto global, lo que reduce los errores de detección
  en comparación con los enfoques que solo consideran regiones
  locales.
- *Simpleza*: El enfoque unificado simplifica el proceso de detección y
  lo hace más fácil de implementar y entrenar.

*** Limitaciones
- *Detección de Objetos Pequeños*: Puede tener dificultades para
  detectar objetos muy pequeños debido a la división de la imagen en
  una cuadrícula de tamaño fijo.
- *Precisión*: Aunque es muy rápida, la precisión de YOLO puede ser
  inferior en comparación con métodos más complejos como Faster R-CNN
  en ciertos escenarios.
- *Trade-off Velocidad-Precisión*: Existe un balance entre la velocidad
  y la precisión; versiones más rápidas de YOLO pueden sacrificar algo
  de precisión por un aumento en la velocidad.

*** Evolución
- *YOLOv2, YOLOv3, YOLOv4, y YOLOv5*: Cada nueva versión de YOLO ha
  introducido mejoras en la arquitectura y el entrenamiento,
  aumentando la precisión y manteniendo o mejorando la velocidad.
- *YOLOv4 y YOLOv5*: Incorporan técnicas avanzadas como mecanismos de
  atención, mejores métodos de anclaje de cajas, y técnicas de
  entrenamiento más robustas, resultando en mejoras significativas en
  la detección de objetos.

YOLO es un algoritmo de detección de objetos eficiente y rápido que
utiliza un enfoque unificado para predecir cajas delimitadoras y
probabilidades de clase en una sola pasada de la red. Es especialmente
útil para aplicaciones en tiempo real donde la velocidad es crítica.

** Ejemplo YOLO

#+BEGIN_SRC python
import cv2
import numpy as np

# Rutas de los archivos de configuración y pesos del modelo YOLOv3
config = "/home/likcos/yolo/yolov3.cfg"
weights = "/home/likcos/yolo/yolov3.weights"
# Cargar los nombres de las etiquetas desde el archivo coco.names
LABELS = open("/home/likcos/yolo/coco.names").read().split("\n")
# Generar colores aleatorios para cada etiqueta
colors = np.random.randint(0, 255, size=(len(LABELS), 3), dtype="uint8")

# Cargar la red YOLO desde los archivos de configuración y pesos
net = cv2.dnn.readNetFromDarknet(config, weights)

# Leer la imagen desde el archivo especificado
image = cv2.imread("/home/likcos/yolo/horse.jpg")
height, width, _ = image.shape

# Crear un blob a partir de la imagen (preprocesamiento)
blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)

# Obtener los nombres de las capas de la red
ln = net.getLayerNames()

# Obtener solo los nombres de las capas de salida
ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]

# Configurar la entrada de la red con el blob
net.setInput(blob)
# Realizar la pasada hacia adelante (forward pass) y obtener las salidas
outputs = net.forward(ln)

# Inicializar listas para las cajas delimitadoras, confidencias y IDs de clase
boxes = []
confidences = []
classIDs = []

# Procesar cada una de las salidas de la red
for output in outputs:
    for detection in output:
        # Obtener las puntuaciones de todas las clases
        scores = detection[5:]
        # Obtener el ID de la clase con la mayor puntuación
        classID = np.argmax(scores)
        # Obtener la confianza (probabilidad) de la clase seleccionada
        confidence = scores[classID]

        # Filtrar detecciones con una confianza baja
        if confidence > 0.5:
            # Escalar las coordenadas de la caja delimitadora a las dimensiones de la imagen original
            box = detection[:4] * np.array([width, height, width, height])
            (x_center, y_center, w, h) = box.astype("int")
            x = int(x_center - (w / 2))
            y = int(y_center - (h / 2))

            # Agregar la caja, confianza e ID de clase a las listas respectivas
            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            classIDs.append(classID)

# Aplicar la supresión de no-máximos para eliminar las cajas redundantes
idx = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.5)
print("idx:", idx)

# Dibujar las cajas finales en la imagen
if len(idx) > 0:
    for i in idx:
        (x, y) = (boxes[i][0], boxes[i][1])
        (w, h) = (boxes[i][2], boxes[i][3])

        # Seleccionar un color para la clase detectada
        color = colors[classIDs[i]].tolist()
        # Crear el texto con la etiqueta y la confianza
        text = "{}: {:.3f}".format(LABELS[classIDs[i]], confidences[i])
        # Dibujar la caja delimitadora y poner el texto en la imagen
        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# Mostrar la imagen resultante
cv2.imshow("Image", image)
cv2.waitKey(0)
cv2.destroyAllWindows()

#+END_SRC

** MobileNet SSD

MobileNet SSD (Single Shot MultiBox Detector) es una arquitectura de
red neuronal profunda utilizada para la detección de objetos en tiempo
real. Combina la eficiencia y velocidad de MobileNet con la precisión
y versatilidad de SSD, lo que la hace ideal para aplicaciones móviles
y embebidas. Aquí tienes una descripción más detallada:

***  MobileNet
- *Arquitectura*: MobileNet es una red neuronal convolucional profunda
  que se caracteriza por su eficiencia computacional y tamaño
  reducido. Utiliza convoluciones separables en profundidad (depthwise
  separable convolutions), lo que reduce significativamente el número
  de parámetros y operaciones de cálculo.
- *Características*:
  - *Convoluciones separables en profundidad*: Descompone la convolución
    estándar en dos operaciones más simples y menos costosas: una
    convolución en profundidad (depthwise convolution) y una
    convolución puntual (pointwise convolution).
  - *Bloques de construcción*: Cada bloque consta de una convolución en
    profundidad seguida de una convolución puntual y una función de
    activación no lineal.
  - *Tamaño reducido*: Diseñada para ser eficiente en dispositivos con
    recursos limitados, como teléfonos móviles y dispositivos IoT.

***  SSD (Single Shot MultiBox Detector)
- *Arquitectura*: SSD es un algoritmo de detección de objetos que
  predice múltiples cajas delimitadoras (bounding boxes) y sus
  probabilidades de clase en una sola pasada a través de la red (de
  ahí el término "Single Shot").
- *Características*:
  - *Predicciones múltiples*: Realiza predicciones en múltiples escalas
    desde varias capas de la red, lo que permite detectar objetos de
    diferentes tamaños.
  - *Mapas de características*: Utiliza mapas de características de
    diferentes niveles de resolución para detectar objetos de
    diferentes tamaños.
  - *Cajas predeterminadas*: Utiliza un conjunto de cajas
    predeterminadas (default boxes) con diferentes proporciones y
    escalas en cada mapa de características.

***  MobileNet SSD
- *Combina lo mejor de ambos mundos*: Integra la eficiencia y velocidad
  de MobileNet con la precisión y capacidad de detección en tiempo
  real de SSD.
- *Aplicaciones*:
  - *Dispositivos móviles*: Ideal para aplicaciones en teléfonos móviles
    y tablets debido a su bajo requerimiento computacional.
  - *IoT y sistemas embebidos*: Perfecta para sistemas embebidos y
    dispositivos IoT donde los recursos de procesamiento son
    limitados.
  - *Drones y robótica*: Utilizada en drones y robots que requieren
    capacidades de detección en tiempo real.

***  Beneficios
- *Eficiencia computacional*: Bajo consumo de energía y recursos,
  permitiendo su implementación en dispositivos con limitaciones de
  hardware.
- *Detección en tiempo real*: Capacidad de procesar y detectar objetos
  rápidamente, lo que es crucial para aplicaciones en tiempo real.
- *Versatilidad*: Puede ser entrenada para detectar una amplia variedad
  de objetos en diferentes entornos y condiciones.

***  Limitaciones
- *Precisión vs. otros modelos*: Aunque es eficiente, su precisión puede
  ser menor en comparación con otros modelos más complejos y pesados.
- *Detección de objetos pequeños*: Puede tener dificultades para
  detectar objetos muy pequeños debido a la reducción de resolución en
  capas profundas.

MobileNet SSD es una arquitectura  eficiente para la
detección de objetos en tiempo real, especialmente adecuada para
dispositivos con recursos limitados.

#+BEGIN_SRC python
import cv2

# Rutas de los archivos de configuración y modelo de MobileNet SSD
prototxt = "/home/likcos/yolo/MobileNetSSD_deploy.prototxt.txt"
model = "/home/likcos/yolo/MobileNetSSD_deploy.caffemodel"

# Diccionario de clases con sus respectivos IDs
classes = {0: "background", 1: "aeroplane", 2: "bicycle",
           3: "bird", 4: "boat",
           5: "bottle", 6: "bus",
           7: "car", 8: "cat",
           9: "chair", 10: "cow",
           11: "diningtable", 12: "dog",
           13: "horse", 14: "motorbike",
           15: "person", 16: "pottedplant",
           17: "sheep", 18: "sofa",
           19: "train", 20: "tvmonitor"}

# Cargar la red MobileNet SSD desde los archivos de configuración y modelo
net = cv2.dnn.readNetFromCaffe(prototxt, model)

# Leer la imagen desde el archivo especificado
image = cv2.imread("/home/likcos/yolo/paj1.png")
height, width, _ = image.shape
# Redimensionar la imagen a 300x300 píxeles, tamaño de entrada esperado por la red
image_resized = cv2.resize(image, (300, 300))

# Crear un blob a partir de la imagen redimensionada (preprocesamiento)
blob = cv2.dnn.blobFromImage(image_resized, 0.007843, (300, 300), (127.5, 127.5, 127.5))
print("blob.shape:", blob.shape)

# Configurar la entrada de la red con el blob
net.setInput(blob)
# Realizar la pasada hacia adelante (forward pass) y obtener las detecciones
detections = net.forward()

# Iterar sobre cada detección
for detection in detections[0][0]:
    print(detection)

    # Filtrar detecciones con una confianza mayor al 45%
    if detection[2] > 0.45:
        # Obtener la etiqueta de la clase
        label = classes[detection[1]]
        print("Label:", label)
        # Escalar las coordenadas de la caja delimitadora a las dimensiones de la imagen original
        box = detection[3:7] * [width, height, width, height]
        x_start, y_start, x_end, y_end = int(box[0]), int(box[1]), int(box[2]), int(box[3])

        # Dibujar la caja delimitadora y las etiquetas en la imagen
        cv2.rectangle(image, (x_start, y_start), (x_end, y_end), (0, 255, 0), 2)
        cv2.putText(image, "Conf: {:.2f}".format(detection[2] * 100), (x_start, y_start - 5), 1, 1.2, (255, 0, 0), 2)
        cv2.putText(image, label, (x_start, y_start - 25), 1, 1.2, (255, 0, 0), 2)

# Mostrar la imagen resultante con las detecciones
cv2.imshow("Image", image)
cv2.waitKey(0)
cv2.destroyAllWindows()


#+END_SRC


* Desafíos y Consideraciones Éticas
  - Desafíos Técnicos: Precisión, grandes conjuntos de datos, computación intensiva.
  - Cuestiones de Privacidad: Preocupaciones sobre reconocimiento facial y vigilancia.
  - Futuro de la Visión por Computadora: Impacto en la sociedad y desarrollo continuo.




* Conclusión y Futuro de la Visión por Computadora
  - Resumen: Repaso de los puntos clave.
  - Futuras Tendencias: Inteligencia artificial, aprendizaje profundo.
  - Preguntas y Discusión: Invitación a participar.


