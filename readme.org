
#+TITLE: Visi√≥n por Computadora 
#+author: Eduardo Alcaraz
#+email: eduardo.ac@morelia.tecnm.mx


* ¬øQue es la visi√≥n por computadora?
[[https://www.dropbox.com/scl/fo/54jqpwdzfnm6l0mdc6hur/h?rlkey=i85skh89ll342evwedk0t3x7r&st=kxmgh8vh&dl=0][Libros]]


La visi√≥n por computadora es un campo de la inteligencia artificial
asociado al an√°lisis de im√°genes y v√≠deos, que incluye un conjunto de
t√©cnicas que otorgan a la computadora la capacidad de *ver* y extraer
informaci√≥n de aquello que se ha visto.

Los sistemas se componen de una c√°mara fotogr√°fica o de v√≠deo y un
software especializado que identifica y clasifica objetos. Son capaces
de analizar im√°genes (fotos, im√°genes, v√≠deos, c√≥digos de barras), as√≠
como caras y emociones.

Para ense√±ar a una computadora a *ver* , se utilizan tecnolog√≠as de
aprendizaje autom√°tico y se recopilan muchos datos que permiten
resaltar caracter√≠sticas y combinaciones de las mismas para
identificar a√∫n m√°s objetos similares.


* Visi√≥n por computadora
#+startup:nlineimages
#+ATTR_LATEX: :width 0.5\textwidth
[[file:img/im1.jpg]]

* Aspectos Claves de la visi√≥n por computadora 

 - *Procesamiento de Im√°genes*: Involucra t√©cnicas para mejorar
   im√°genes digitales, extraer informaci√≥n √∫til, realizar ajustes como
   correcci√≥n de color, reducci√≥n de ruido, y m√°s.

 - *Reconocimiento de Patrones*: Se refiere a la identificaci√≥n de
   patrones, formas y caracter√≠sticas en las im√°genes. Esto puede
   incluir el reconocimiento de rostros, objetos, escenas, gestos,
   etc.

 - *Segmentaci√≥n de Im√°genes*: Se trata de dividir una imagen en
   diferentes partes o segmentos, a menudo para aislar regiones o
   elementos de inter√©s.

 - *Detecci√≥n y Seguimiento de Objetos*: Implica identificar y seguir
   objetos a lo largo del tiempo en una serie de im√°genes o v√≠deos.

 - *Reconstrucci√≥n 3D*: Consiste en crear representaciones
   tridimensionales de un objeto o escena a partir de im√°genes
   bidimensionales.

 - *Visi√≥n Artificial en Rob√≥tica*: Aplicaci√≥n de la visi√≥n por
   computadora en robots para la navegaci√≥n, manipulaci√≥n de objetos y
   otras tareas.

 - *Interpretaci√≥n de Escenas*: Comprende el an√°lisis de escenas
   completas en t√©rminos de identificaci√≥n de objetos, su disposici√≥n
   espacial, interacciones, y el contexto general.

La visi√≥n por computadora se utiliza en una variedad de aplicaciones,
que van desde la seguridad y vigilancia hasta la medicina, pasando por
los veh√≠culos aut√≥nomos, la inspecci√≥n industrial, la gesti√≥n de
contenido digital y la interacci√≥n hombre-m√°quina. Este campo se
beneficia enormemente de los avances en el aprendizaje autom√°tico y el
aprendizaje profundo, permitiendo el desarrollo de sistemas m√°s
precisos y eficientes para el an√°lisis visual.

* Libros de Visi√≥n por Computadora 
- Geometr√≠a de Vistas M√∫ltiples en Visi√≥n por Computadora *Richard
  Hartley*
- Visi√≥n por Computadora Modelos, Aprendizaje e Inferencia *Simon
  J. D. Prince*
- Visi√≥n por Computadora Un Enfoque Moderno *David A. Forsyth*
- Aprendizaje Profundo Pr√°ctico para la Nube, M√≥vil y Edge Proyectos
  Reales de IA y Visi√≥n por Computadora Usando Python, Keras y
  TensorFlow *Anirudh Koul*
- Aprendiendo OpenCV 4 Visi√≥n por Computadora con Python 3 Entiende
  las herramientas, t√©cnicas y algoritmos para la visi√≥n por
  computadora y el aprendizaje autom√°tico *Joseph Howse*

* Tecnolog√≠as y Herramientas
  - Lenguajes de Programaci√≥n: Python, C++, C.
  - Bibliotecas y Frameworks: OpenCV, TensorFlow, PyTorch.

* Instalaci√≥n Python Opencv


** Instalaci√≥n python Opencv Windows


- *Paso 1: Instalar Python 3*
   - Python se puede descargar desde la p√°gina oficial: [[https://www.python.org/downloads/][Python Downloads]].
   - Aseg√∫rate de marcar la opci√≥n "Add Python 3.x to PATH" durante la instalaci√≥n.

- *Paso 2: Verificar la Instalaci√≥n de Python*
   - Abre la Terminal de Comandos (Command Prompt) y ejecuta:
     #+BEGIN_SRC bash
     python --version
     #+END_SRC
   - Esto deber√≠a mostrar la versi√≥n de Python instalada.

- *Paso 3: Actualizar pip (Gestor de Paquetes de Python)*
   - En la Terminal de Comandos, ejecuta:
     #+BEGIN_SRC bash
     python -m pip install --upgrade pip
     #+END_SRC

- *Paso 4: Instalar OpenCV*
   - Utiliza pip para instalar OpenCV. En la Terminal de Comandos, ejecuta:
     #+BEGIN_SRC bash
     pip install opencv-python
     #+END_SRC
   - Si necesitas las funcionalidades adicionales de OpenCV, instala tambi√©n opencv-contrib-python:
     #+BEGIN_SRC bash
     pip install opencv-contrib-python
     #+END_SRC

- *Paso 5: Verificar la Instalaci√≥n de OpenCV*
   - Para verificar que OpenCV est√° instalado, abre un int√©rprete de Python y ejecuta:
     #+BEGIN_SRC python
     import cv2
     print(cv2.__version__)
     #+END_SRC
   - Si se muestra la versi√≥n de OpenCV sin errores, la instalaci√≥n
     fue exitosa.

- *Notas Finales*
   - Es recomendable reiniciar el sistema despu√©s de instalar Python
     para asegurar que todos los cambios de configuraci√≥n se apliquen
     correctamente.
   - Puede ser √∫til trabajar en un entorno virtual para proyectos de
     Python para gestionar las dependencias de manera m√°s eficiente.

	


** Instalaci√≥n python Opencv Mac 

Instalar Python 3 y OpenCV en macOS es un proceso bastante sencillo. A
continuaci√≥n, se muestran los pasos para realizar esta instalaci√≥n.

- *Paso 1: Instalar Python 3*
macOS viene con Python 2.7 instalado por defecto, pero se recomienda usar Python 3 para proyectos nuevos.

   #+BEGIN_SRC bash
   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
   brew install python3
   #+END_SRC
   Verificar la versi√≥n de Python:
   #+BEGIN_SRC bash
   python3 --version
   #+END_SRC

- *Paso 2: Instalar pip*
   pip es el gestor de paquetes de Python y generalmente viene instalado con Python 3.
   Verificar si pip est√° instalado:
   #+BEGIN_SRC bash
   pip3 --version
   #+END_SRC
   Si pip no est√° instalado:
   #+BEGIN_SRC bash
   sudo easy_install pip
   #+END_SRC

- *Paso 3: Instalar OpenCV*
   Puedes instalar OpenCV para Python utilizando pip.
   Instalaci√≥n b√°sica:
   #+BEGIN_SRC bash
   pip3 install opencv-python
   #+END_SRC
   Instalar con funcionalidades adicionales:
   #+BEGIN_SRC bash
   pip3 install opencv-contrib-python
   #+END_SRC

- *Paso 4: Verificar la Instalaci√≥n de OpenCV*
   Para comprobar que OpenCV est√© correctamente instalado:
   #+BEGIN_SRC python
   import cv2
   print(cv2.__version__)
   #+END_SRC

- *Notas Finales*
  - Es una buena pr√°ctica trabajar en un entorno virtual para proyectos de Python.
  - Aseg√∫rate de que tu sistema macOS est√© actualizado.





** Instalaci√≥n python Opencv Linux

Instalar Python 3 y OpenCV en un sistema Linux generalmente es un
proceso sencillo. A continuaci√≥n, te presento los pasos gen√©ricos para
la mayor√≠a de las distribuciones de Linux. Ten en cuenta que estos
pasos pueden variar ligeramente dependiendo de la distribuci√≥n
espec√≠fica que est√©s utilizando (como Ubuntu, Fedora, etc.).

- *Instalar Python 3*

La mayor√≠a de las distribuciones modernas de Linux ya vienen con
Python 3 instalado. Puedes verificar si Python 3 est√° instalado y su
versi√≥n usando el siguiente comando en la terminal:

#+BEGIN_SRC shell
python3 --version
#+END_SRC

Si Python 3 no est√° instalado o deseas una versi√≥n m√°s reciente,
puedes instalarlo a trav√©s del gestor de paquetes de tu distribuci√≥n:

- *En distribuciones basadas en Debian (como Ubuntu):*

  #+BEGIN_SRC shell
sudo apt update
sudo apt install python3
#+END_SRC

- *En distribuciones basadas en Red Hat (como Fedora):*

#+BEGIN_SRC shell
 sudo dnf install python3
#+END_SRC

- *Instalar pip (Gestor de Paquetes de Python)*

*pip* es el gestor de paquetes para Python y se utiliza para instalar paquetes de Python como OpenCV. Puedes instalar `pip` con el siguiente comando:

- *En Ubuntu y otras distribuciones basadas en Debian:*

#+BEGIN_SRC bash
 sudo apt install python3-pip
#+END_SRC
 
- En Fedora y distribuciones basadas en Red Hat:

  sudo dnf install python3-pip


- *Instalar OpenCV*

Una vez que tengas Python 3 y pip instalados, puedes instalar OpenCV. El paquete `opencv-python` proporciona enlaces a las bibliotecas de OpenCV y es el m√©todo m√°s f√°cil de instalar OpenCV para Python. Ejecuta el siguiente comando:

#+BEGIN_SRC bash
pip3 install opencv-python
#+END_SRC

Si necesitas los m√≥dulos adicionales (que incluyen algoritmos patentados), puedes instalar `opencv-contrib-python`:

#+BEGIN_SRC bash
pip3 install opencv-contrib-python
#+END_SRC

- *Verificar la Instalaci√≥n*

Para verificar que OpenCV est√° correctamente instalado, puedes hacer lo siguiente:

1. Abre una terminal y escribe `python3` para entrar en el int√©rprete interactivo de Python.

2. En el int√©rprete, escribe:

#+BEGIN_SRC python
 import cv2
 print(cv2.__version__)
  
#+END_SRC
 
   Si no hay errores y se muestra la versi√≥n de OpenCV, significa que la instalaci√≥n fue exitosa.

- *Notas Adicionales*

   - Si est√°s trabajando en un entorno de desarrollo profesional o experimental, es una buena pr√°ctica usar entornos virtuales para gestionar las dependencias de Python. Puedes usar herramientas como `venv` o `conda` para crear entornos virtuales.

   - Aseg√∫rate de que tu sistema est√© actualizado antes de comenzar la instalaci√≥n.

   - Los pasos exactos pueden variar ligeramente dependiendo de la versi√≥n y el tipo de tu distribuci√≥n de Linux. 


* Aplicaciones de la Visi√≥n por Computadora
  - Reconocimiento Facial: Uso en seguridad y dispositivos m√≥viles.
  - Veh√≠culos Aut√≥nomos: Navegaci√≥n y detecci√≥n de obst√°culos.
  - An√°lisis M√©dico de Im√°genes: Aplicaci√≥n en diagn√≥stico y an√°lisis.

* Programaci√≥n 

** Cargar imagen

   #+BEGIN_SRC python :results output
import cv2 as cv 
img = cv.imread('/home/likcos/Im√°genes/tr.png', 1)
cv.imshow('ejemplo', img)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC
   #+RESULTS:
** Modelos de Color

#+BEGIN_SRC python :results output :tangle src/canales.py 
import cv2 as cv 
img = cv.imread('/home/likcos/Im√°genes/tr.png', 1)
imgHSV = cv.cvtColor(img, cv.COLOR_BGR2RGB)        
cv.imshow('ejemplo', img)
cv.imshow('ejemploGris', imgHSV)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

#+RESULTS:

** Canales de color
#+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np 
img = cv.imread('img/tr.png')
img2 = np.zeros(img.shape[:2], dtype=np.uint8)
#print(img.shape[1])
b,g,r =cv.split(img)
rb=cv.merge([g,b,r])
rg=cv.merge([img2,g,img2])
rr=cv.merge([img2,img2,r])
cv.imshow('img', img)
cv.imshow('b',b)
cv.imshow('g',g)
cv.imshow('r',r)
cv.imshow('rb',rb)
cv.imshow('gr',rg)
cv.imshow('rr',rr)
cv.waitKey(0)
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: 635

** Segmentaci√≥n de color 

#+BEGIN_SRC python
import cv2 as cv
img = cv.imread('img/mango.jpg',1)
imghsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
#imgRGB = cv.cvtColor(img, cv.COLOR_BGR2RGB)
ubb=(24,100, 100)
uba=(32, 255,255)
#ubb1=(28,100, 100)
#uba1=(32, 255,255)
mask = cv.inRange(imghsv, ubb, uba)
#mask2 = cv.inRange(imghsv, ubb1, uba1)
#mask = mask1 + mask2
res = cv.bitwise_and(img, img, mask=mask)
cv.imshow('img', img)
cv.imshow('Resultado',res )
cv.imshow('mask', mask)
cv.waitKey(0)
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: None


** Filtros de convoluci√≥n 
#+BEGIN_SRC python
import cv2 as cv 
import numpy as np 

img = cv.imread('/home/likcos/Im√°genes/mo1.png',0)
mtz = np.array([[-1,-2,-1],
                [0,0,0],
                [1,2,1]])
resultado = cv.filter2D(img, -1, mtz)
cv.imshow('marcoc', resultado)
cv.imshow('marco', img)
cv.waitKey(0)
cv.destroyAllWindows()


#+END_SRC

#+RESULTS:
: None




** Transformaciones Geom√©tricas 

En el procesamiento de im√°genes y visi√≥n por computadora, las
transformaciones afines son operaciones fundamentales que permiten
manipular la geometr√≠a de las im√°genes. Estas transformaciones
incluyen translaci√≥n, escalamiento, rotaci√≥n y cizallamiento, y son
ampliamente utilizadas para tareas como la alineaci√≥n de im√°genes, la
correcci√≥n de distorsiones y la creaci√≥n de efectos visuales.

Una transformaci√≥n af√≠n es una funci√≥n geom√©trica que preserva puntos,
l√≠neas rectas y planos. En el contexto de im√°genes, esto significa que
las transformaciones afines mantienen la colinealidad y la proporci√≥n
de las distancias entre los puntos. Matem√°ticamente, una
transformaci√≥n af√≠n se representa mediante una matriz $3√ó3$ en
coordenadas homog√©neas.

*** Traslaci√≥n
La traslaci√≥n desplaza cada punto de la imagen una cierta distancia en
las direcciones ùë• x y ùë¶ y. Es una de las transformaciones m√°s simples
y se utiliza com√∫nmente para mover una imagen de un lugar a otro sin
cambiar su forma o tama√±o.  Una translaci√≥n la podemos hacer
simplemente asumiendo que nuevas coordenadas $\hat{x} = x + t_x$
$\hat{y} = y + t_y$ les sumamos un valor $t_x$ p $t_y$ seg√∫n
corresponda. La f√≥rmula en coordenadas homog√©neas es:

$$
\begin{pmatrix}
\hat{x}\\ 
\hat{y}\\
1
\end{pmatrix}
$$
$$
=
\begin{pmatrix}
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
x \\
y \\
1
\end{pmatrix}
$$


#+BEGIN_SRC python :tangle src/traslacion.py
import cv2 as cv
import numpy as np
img = cv.imread('/home/likcos/Im√°genes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*4, w*4, 1) , dtype = "uint8")
print("Valores " + str(img.shape[:2]))
for i in range(h):
    for j in range(w):
        img2[int(3*(i+20)),int(j+10)]=img[i,j]

cv.imshow('imagen', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: None


*** Escalamiento 
	 El escalamiento puede entenderse como hacer una figura geom√©trica
	 cambie su tama√±o o cambie su escala. Un escalamiento en x lo
	 podemos representar  como $\hat{x} = x$,  $s_x$ y en y como
	 $\hat{y} = y$,  $s_y$ En coordenada homog√©neas se puede expresar como 

$$
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&\\
 1
 \end{pmatrix}
 = 
 \begin{pmatrix}
 s_x & 0& 0\\
 0 & s_y & 0&\\
 0 & 0 & 1
 \end{pmatrix}
\begin{pmatrix}
 x\\
 y&\\
 1
 \end{pmatrix}
$$	

 #+BEGIN_SRC python :results output :tangle src/escalamiento.py
import cv2 as cv
import numpy as np
img = cv.imread('/home/likcos/Im√°genes/mo1.png',0)
h,w = img.shape[:2]
print(h, w)
img2 = np.zeros((h*3, w*3) , dtype = "uint8")
print("Valores " + str(img.shape[:2]))
for i in range(h):
    for j in range(w):
        img2[int(i*0.5),int(j*0.5)]=img[i,j]

cv.imshow('imagen', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

 #+END_SRC

 #+RESULTS:
 : 441 524
 : Valores (441, 524)

*** Rotaci√≥n 

 Considerando el caso de un punto que rota respecto a un punto
 fijo. Las coordenadas x y y, en forma polar las podemos obtener como $x=r$ 
 $cos(\theta) y y = r sen(\theta)$. Si consideramos que esta gira un √°ngulo $\theta$    
 entonces podemos representar esta rotaci√≥n en forma polar. 
 
$$
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
 r cos(\alpha + \theta)\\
 r sen(\alpha + \theta)
 \end{pmatrix}
 = 
 \begin{pmatrix}
 r cos(\alpha + \theta) - r sin(\alpha) sin(\theta) \\
 r sen(\alpha + \theta) + r sin(\alpha) con(\theta)
 \end{pmatrix}
$$
 
$$
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
 x cos(\theta) - y sin(\theta) \\
 x sen(\theta) + y cos(\theta)
 \end{pmatrix}
 \begin{pmatrix}
 \hat{x}\\
 \hat{y}&
 \end{pmatrix}
 = 
 \begin{pmatrix}
  cos(\theta) &-  sin(\theta) \\
  sen(\theta) &  cos(\theta)
 \end{pmatrix}
 \begin{pmatrix}
 x \\
 y 
 \end{pmatrix}
 xcos(\theta) - ysin(\theta), xsen(\theta) + ycos(\theta)
$$
 
#+BEGIN_SRC python :results output :tangle src/rotacion.py
import cv2 as cv
import math
import numpy as np 

img = cv.imread('/home/likcos/Im√°genes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*3, w*3), dtype = "uint8")
for i in range(h):
    for j in range(w):
        img2[int(i*math.cos(math.radians(30))-j*math.sin(math.radians(30))),
             int(i*(math.sin(math.radians(30)))+j*math.cos(math.radians(30)))]=img[i,j]
cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()
 #+END_SRC

 #+RESULTS:

*** Cizallamiento 

   El cizallamiento es una transformaci√≥n dada por la matriz, donde $c_x$
   es el √°ngulo de cizallamiento respecto al eje x

   \begin{equation}
   C_x
   = 
   \begin{pmatrix}
   1 & tg(C_x)& 0\\
   0 & 1 & 0&\\
   0 & 0 & 1
   \end{pmatrix}

   \end{equation}



   #+BEGIN_SRC python :results output
import cv2 as cv
import math
import numpy as np 

img = cv.imread('/home/likcos/Im√°genes/mo1.png',0)
h,w = img.shape[:2]
img2 = np.zeros((h*2, w*2), dtype = "uint8")
matz = np.array([[1,1,1],[1,1,1],[1,1,1]])
for i in range(h):
    for j in range(w):
        img2[int(i*2) ,int(j*2)]=img[i,j]
res = cv.filter2D(img2, -1, matz)
cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.imshow('imagen3', res)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

   #+RESULTS:

*** Traslaci√≥n Opencv  WarpAffine Afine

   #+BEGIN_SRC python
import cv2 as cv
import numpy as np 

img = cv.imread('/home/likcos/Im√°genes/mo1.png')
h,w = img.shape[:2]
mw = np.float32([[1,0,10],[0,1,10]])
img2 = cv.warpAffine(img,mw,(h,w))

cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()


   #+END_SRC

   #+RESULTS:
   : None

**** Rotaci√≥n Opencv WarpAffine + getRotationMatrix2D

   #+BEGIN_SRC python
import cv2 as cv
import numpy as np 

img = cv.imread('/home/likcos/Im√°genes/mo1.png')
h,w = img.shape[:2]

mw = cv.getRotationMatrix2D((h//2, w//2),30,-1)
img2 = cv.warpAffine(img,mw,(h,w))

cv.imshow('imagen1', img)
cv.imshow('imagen2', img2)
cv.waitKey(0)
cv.destroyAllWindows()
   #+END_SRC

   #+RESULTS:
   : None

*** Primitivas de Dibujo

   #+BEGIN_SRC python
import cv2 as cv 
import numpy as np 
img = 58*np.ones((1000,1000,3), np.uint8)
cv.line(img,(0,0), (100,100), (23, 189, 200), 3)
cv.rectangle(img, (40,40), (80,80), (1,65,90), -1)
cv.circle(img, (100,100), 50, (45, 190,200),-1)
cv.circle(img, (100,100), 45, (45, 200,90),-1)
cv.ellipse(img,(256,256),(100,50),0,0,180,255,-1)
pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)
pts = pts.reshape((-1,1,2))
cv.polylines(img,[pts],True,(0,255,255))
cv.imshow('marco',img)
cv.waitKey(0)
cv.destroyAllWindows()

   #+END_SRC

   #+RESULTS:


   #+begin_src python :results output
import cv2 as cv 
import numpy as np 
import math

Pi = 3.1416
img = 255 * np.ones((500, 500, 3 ), np.uint8)

for i in range(360):
    #img = 255 * np.ones((500, 500, 3 ), np.uint8)
    h, w = img.shape[:2] 
   
    #x = int(h/2) + int(100* math.sin(6*(i*(Pi/180))))*math.sin(i*Pi/180)
    #y = int(w/2) + int(100* math.sin(6*(i*(Pi/180))))*math.cos(i*Pi/180)
    
    #xx = int(h/3) + int(100* (-1+math.cos(i*(Pi/180)))*math.sin(i*Pi/180))
    #yy = int(w/3) + int(100* (-1+math.cos(i*(Pi/180)))*math.cos(i*Pi/180))

    xx = int(h/2) + int(100* (math.cos(1*(i*(Pi/180))))*(-1*(math.cos(80*(i*Pi/180)))))
    yy = int(w/2) + int(100* (math.sin(1*(i*(Pi/180))))*(-1*(math.sin(80*(i*Pi/180)))))

    #cv.circle(img, (int(x) , int(y)), 3, (0,i,0), -1)
    #cv.circle(img, (int(y) , int(x)), 3, (i,0,0), -1)
    cv.circle(img, (int(xx) , int(yy)), 1, (0,0,i), -1)
    #cv.imwrite('resultado'+str(i)+'.jpg',img)

    cv.imshow('imagen', img)
    cv.waitKey(10)

cv.imshow('imagen', img)
cv.imwrite('resultado.jpg',img)
cv.waitKey(0)
cv.destroyAllWindows()
   
   #+end_src



*** Flujo √≥ptico 

El flujo √≥ptico es un concepto en visi√≥n por computadora y
procesamiento de im√°genes que se refiere al patr√≥n de movimiento
aparente de los objetos, las superficies y los bordes en una escena
visual causado por el movimiento relativo entre un observador y la
escena. La idea es estimar c√≥mo se mueven los puntos de una imagen
entre dos cuadros consecutivos de un video o entre dos im√°genes
tomadas en momentos diferentes.

*Conceptos Clave del Flujo √ìptico:* Vector de Movimiento: Cada punto en
la imagen tiene asociado un vector que indica la direcci√≥n y la
magnitud del movimiento de ese punto entre dos cuadros.

*Consistencia de Brillo*: Se asume que el brillo (intensidad) de un
punto en la imagen permanece constante entre cuadros consecutivos, lo
que permite relacionar los puntos en diferentes cuadros.

*Restricciones Espaciales y Temporales*: Se considera que los puntos
vecinos en una imagen tienden a tener movimientos similares, y este
movimiento cambia suavemente a lo largo del tiempo.

*M√©todos para Calcular el Flujo √ìptico*: M√©todos Basados en Gradientes:
Utilizan las variaciones del brillo y los gradientes de la imagen para
calcular el movimiento. Un ejemplo es el algoritmo de Lucas-Kanade,
que asume que el flujo √≥ptico es esencialmente constante en una
peque√±a ventana de la imagen.

*M√©todos Basados en Bloques*: Comparan bloques (peque√±as √°reas) de un
cuadro con los del cuadro siguiente, buscando el bloque que mejor se
ajuste. Esto se hace por ejemplo en la t√©cnica de coincidencia de
bloques.

*M√©todos Basados en Caracter√≠sticas*: Identifican caracter√≠sticas
distintivas en las im√°genes (como esquinas o bordes) y rastrean c√≥mo
se mueven estas caracter√≠sticas entre los cuadros.

*M√©todos Basados en Aprendizaje Profundo*: Utilizan redes neuronales
para aprender y predecir el movimiento en secuencias de im√°genes.

*Aplicaciones del Flujo √ìptico*:
*Seguimiento de Objetos*: Rastrear el movimiento de objetos en videos.
*Estabilizaci√≥n de Video*: Corregir la sacudida en las grabaciones de video.
*Reconstrucci√≥n de Escenas 3D*: Ayuda a entender la estructura tridimensional del entorno.
*An√°lisis de Movimiento*: En deportes o medicina para analizar movimientos del cuerpo humano.

*Limitaciones:* 
Sensible a cambios de iluminaci√≥n.  No funciona bien en
escenas con mucho movimiento o sin texturas.  
La asunci√≥n de consistencia de brillo no siempre es v√°lida.  
El flujo √≥ptico es una herramienta poderosa en visi√≥n por computadora, pero su precisi√≥n y
eficacia dependen en gran medida del m√©todo espec√≠fico utilizado y de
las caracter√≠sticas de la escena que se est√° analizando.

#+BEGIN_SRC python :results output
import numpy as np 
import cv2 as cv

cap = cv.VideoCapture(0)


lkparm =dict(winSize=(15,15), maxLevel=2,
             criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03)) 


_, vframe = cap.read()
vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
p0 = np.array([(100,100), (200,100), (300,100), (400,100), (500,100),
               (100,200), (200,200), (300,200), (400,200), (500,200),
               (100,300), (200,300), (300,300), (400,300), (500,300),
               (100,400), (200,400), (300,400), (400,400), (500,400)])

p0 = np.float32(p0[:, np.newaxis, :])

mask = np.zeros_like(vframe)
cad =''

while True:
    _, frame = cap.read()
    fgris = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    p1, st, err = cv.calcOpticalFlowPyrLK(vgris, fgris, p0, None, **lkparm) 

    if p1 is None:
        vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
        p0 = np.array([(100,100), (200,100), (300,100), (400,100) ])
        p0 = np.float32(p0[:, np.newaxis, :])
        mask = np.zeros_like(vframe)
        cv.imshow('ventana', frame)
    else:
        bp1 = p1[st ==1]
        bp0 = p0[st ==1]
        
        for i, (nv, vj) in enumerate(zip(bp1, bp0)):
            a, b = (int(x) for x in nv.ravel())
            c, d = (int(x) for x in vj.ravel())
            dist = np.linalg.norm(nv.ravel() - vj.ravel())

            #print(i, dist)
            
            
            
            frame = cv.line(frame, (c,d), (a,b), (0,0,255), 2)
            frame = cv.circle(frame, (c,d), 2, (255,0,0),-1)
            frame = cv.circle(frame, (a,b), 3, (0,255,0),-1)
        cv.imshow('ventana', frame)

        vgris = fgris.copy()

        if(cv.waitKey(1) & 0xff) == 27:
            break

cap.release()
cv.destroyAllWindows()
#+END_SRC

#+BEGIN_SRC python :results output
import numpy as np 
import cv2 as cv

cap = cv.VideoCapture(0)


lkparm =dict(winSize=(15,15), maxLevel=2,
             criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03)) 


_, vframe = cap.read()
vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
p0 = np.array([(100,100), (200,100), (300,100), (400,100)])
p0 = np.float32(p0[:, np.newaxis, :])

mask = np.zeros_like(vframe)
cad =''

while True:
    _, frame = cap.read()
    fgris = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    p1, st, err = cv.calcOpticalFlowPyrLK(vgris, fgris, p0, None, **lkparm) 

    if p1 is None:
        vgris = cv.cvtColor(vframe, cv.COLOR_BGR2GRAY)
        p0 = np.array([(100,100), (200,100), (300,100), (400,100) ])
        p0 = np.float32(p0[:, np.newaxis, :])
        mask = np.zeros_like(vframe)
        cv.imshow('ventana', frame)
    else:
        bp1 = p1[st ==1]
        bp0 = p0[st ==1]
        
        for i, (nv, vj) in enumerate(zip(bp1, bp0)):
            a, b = (int(x) for x in nv.ravel())
            c, d = (int(x) for x in vj.ravel())
            dist = np.linalg.norm(nv.ravel() - vj.ravel())

            #print(i, dist)
            
            if i == 0 and dist > 30 :
                cad = cad + '0' 
            elif i == 1 and dist > 30 :
                cad = cad + '1' 
            elif i == 2 and dist > 30 :    
                print('2', dist)
                cad = cad + '2' 
            elif i== 3 and dist > 30 :
                cad= cad+'3' 
           
            frame = cv.putText(frame, cad, (50,50),
                               cv.FONT_HERSHEY_SIMPLEX, 1 , (255,0,0),2, cv.LINE_AA)    
            frame = cv.putText(frame, str(i), (c,d),
                               cv.FONT_HERSHEY_SIMPLEX, 1 , (255,0,0),2, cv.LINE_AA)    
            
            frame = cv.line(frame, (c,d), (a,b), (0,0,255), 2)
            frame = cv.circle(frame, (c,d), 2, (255,0,0),-1)
            frame = cv.circle(frame, (a,b), 3, (0,255,0),-1)
        cv.imshow('ventana', frame)

        vgris = fgris.copy()

        if(cv.waitKey(1) & 0xff) == 27:
            break

cap.release
cv.destroyAllWindows()
  #+END_SRC


#+BEGIN_SRC python :results output
import numpy as np
import cv2 as cv

# Inicializa la captura de video desde la c√°mara
cap = cv.VideoCapture(0)

# Par√°metros para el flujo √≥ptico de Lucas-Kanade
lk_params = dict(winSize=(15, 15), maxLevel=2,
                 criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))

# Captura el primer frame y convi√©rtelo a escala de grises
ret, old_frame = cap.read()
old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)

# Define el punto inicial para el seguimiento (en este caso, el centro del rect√°ngulo)
start_point = np.array([[old_frame.shape[1] // 2, old_frame.shape[0] // 2]], np.float32)


while True:
    # Captura un nuevo frame
    ret, frame = cap.read()
    if not ret:
        break
    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)

    # Calcula el flujo √≥ptico
    new_points, status, error = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, start_point, None, **lk_params)

    # Selecciona los puntos buenos
    if status[0] == 1:
        good_new = new_points[0]
        good_old = start_point[0]

        # Dibuja el circulo en la nueva posici√≥n
        a, b = good_new.ravel()
        start_point = np.array([[a, b]], np.float32)
        center = (int(a), int(b))
        frame = cv.circle(frame, center, 50, (0, 255, 0), -1)

    # Muestra el frame con el rect√°ngulo
    cv.imshow('Flujo Optico', frame)

    # Actualiza el frame anterior y los puntos
    old_gray = frame_gray.copy()

    # Salir del bucle si se presiona la tecla 'Esc'
    if cv.waitKey(1) & 0xFF == 27:
        break

# Libera los recursos
cap.release()
cv.destroyAllWindows()
#+END_SRC


  
#+RESULTS:
#+begin_example
2 120.083694
2 120.879524
2 140.26334
2 186.67227
2 137.3906
2 62.329464
2 64.929184
2 69.902626
2 54.96874
2 52.69248
2 60.124416
2 63.391445
2 89.591415
2 56.381046
2 62.464
2 43.80231
2 39.35062
2 49.12454
2 46.783375
2 38.045883
2 33.4635
2 31.740929
2 57.095345
2 41.95649
2 82.12661
2 58.450493
2 50.61094
2 56.976192
2 52.000736
2 87.50451
2 53.20077
2 67.31267
2 69.54097
2 63.143032
2 35.948177
2 95.95501
2 74.23872
2 76.64836
2 68.564
2 87.3111
2 74.414635
2 72.70697
2 61.554096
#+end_example


*** V√≠deo

**** Cargar v√≠deo simple opencv 
  #+BEGIN_SRC python :results output

import cv2 as cv 

cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        cv.imshow('video', img)
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()
  #+END_SRC

 #+RESULTS:

**** Divisi√≥n de canales de color en v√≠deo
 #+BEGIN_SRC python
import cv2 as cv 
import numpy as np
cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        img2 = np.zeros(img.shape[:2], dtype=np.uint8)
        b,g,r =cv.split(img)
        rb=cv.merge([g,r,b])
        rg=cv.merge([r,g,b])
        rr=cv.merge([b,r,r])
        #imgGris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)        
        cv.imshow('b',rb)
        cv.imshow('g',rg)
        cv.imshow('r',rr)
        cv.imshow('video', img)
        #cv.imshow('videogris', imgGris)
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()
 #+END_SRC

**** Seguimiento por color 

#+BEGIN_SRC python
import cv2 as cv 

cap = cv.VideoCapture(0)
while(True):
    ret, img = cap.read()
    if ret == True:
        #cv.imshow('video', img)
        imghsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        ubb=(100,100, 100)
        uba=(130, 255,255)
        mask = cv.inRange(imghsv, ubb, uba)
        res = cv.bitwise_and(img, img, mask=mask)
        cv.imshow('resultado', res)
        cv.imshow('hsv', imghsv)
        cv.imshow('mask', mask)
        
        k =cv.waitKey(1) & 0xFF
        if k == 27 :
            break
    else:
        break
cap.release()
cv.destroyAllWindows()




#+END_SRC

#+RESULTS:
: None




*** Haarcascades 
Los Haar Cascades son una t√©cnica utilizada en el campo de la visi√≥n
por computadora para la detecci√≥n de objetos. Fueron introducidos por
Paul Viola y Michael Jones en su art√≠culo seminal "Rapid Object
Detection using a Boosted Cascade of Simple Features" en 2001. Esta
t√©cnica es particularmente conocida por su eficacia en la detecci√≥n de
rostros, aunque puede ser utilizada para detectar otros tipos de
objetos.

#+startup: inlineimages
#+ATTR_LATEX: :width 0.3\textwidth
[[file:img/cascade.png]]

**** Conceptos Clave: 
Caracter√≠sticas de Haar: Son patrones visuales
 simples que se pueden calcular r√°pidamente en una imagen. Estas
 caracter√≠sticas se asemejan a peque√±as versiones de n√∫cleos de wavelet
 de Haar y son utilizadas para capturar la presencia de bordes, cambios
 de textura, y otras propiedades visuales.

 
**** Im√°genes Integrales: 
Para acelerar el c√°lculo de las caracter√≠sticas
 de Haar, se utiliza un concepto llamado imagen integral. Una imagen
 integral permite calcular la suma de los valores de los p√≠xeles en
 cualquier √°rea rectangular de la imagen en tiempo constante.

****  Adaboost: 
Es un m√©todo de aprendizaje autom√°tico utilizado para
 mejorar la eficiencia de la detecci√≥n. Selecciona un peque√±o n√∫mero
 de caracter√≠sticas cr√≠ticas de un conjunto m√°s grande y construye
 clasificadores "d√©biles". Luego, estos se combinan en un clasificador
 m√°s fuerte y eficiente.

****  Cascadas: 
En lugar de aplicar todas las caracter√≠sticas a una ventana de la
imagen, se organizan en una secuencia de etapas (cascadas). Cada etapa
tiene su propio clasificador (hecho con Adaboost) y solo pasa las
ventanas de la imagen que parecen prometedoras. Esto reduce
significativamente el tiempo de c√°lculo, ya que muchas ventanas no
pasan las primeras etapas.

 *Proceso de Detecci√≥n*: 
Pre-procesamiento: Se convierte la imagen en
 escala de grises y se crea su imagen integral.

 *Aplicaci√≥n de las Caracter√≠sticas*: Se desplaza una ventana sobre la
 imagen, y en cada posici√≥n, se calculan las caracter√≠sticas de Haar.

 *Clasificaci√≥n en Cascada*: Cada ventana es evaluada a trav√©s de la
 cascada de clasificadores. Si una ventana falla en alguna etapa, se
 descarta. Si pasa todas las etapas, se considera como una detecci√≥n.

 *Post-procesamiento*: Finalmente, se pueden aplicar t√©cnicas como la
 supresi√≥n de no m√°ximos para reducir falsos positivos y mejorar la
 precisi√≥n.

 *Aplicaciones*: Detecci√≥n de rostros en im√°genes y videos.  Detecci√≥n
 de peatones u otros objetos en sistemas de vigilancia.  Aplicaciones
 de realidad aumentada.  Es importante mencionar que, aunque los Haar
 Cascades fueron revolucionarios en su momento, han sido superados en
 precisi√≥n y velocidad por t√©cnicas m√°s modernas de aprendizaje
 profundo. Sin embargo, siguen siendo utilizados debido a su
 simplicidad y bajo requerimiento de recursos computacionales.

**** Ejemplo de un Haarcascade

https://github.com/opencv/opencv/tree/master/data/haarcascades

https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html

https://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html

https://amin-ahmadi.com/cascade-trainer-gui/
#+BEGIN_SRC python
import numpy as np
import cv2 as cv
import math 

rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)
i = 0  
while True:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gray, 1.3, 5)
    for(x, y, w, h) in rostros:
       #frame = cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)
       frame2 = frame[ y:y+h, x:x+w]
        #frame3 = frame[x+30:x+w-30, y+30:y+h-30]
       frame2 = cv.resize(frame2, (100, 100), interpolation=cv.INTER_AREA)
       cv.imwrite('/home/likcos/pruebacaras/juan/juan'+str(i)+'.jpg', frame2)
       cv.imshow('rostror', frame2)
    cv.imshow('rostros', frame)
    i = i+1
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
import cv2 as cv 

rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)

while True:
    ret, img = cap.read()
    gris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gris, 1.3, 5)
    for(x,y,w,h) in rostros:
        res = int((w+h)/8)
        img = cv.rectangle(img, (x,y), (x+w, y+h), (234, 23,23), 2)
        img = cv.rectangle(img, (x,int(y+h/2)), (x+w, y+h), (0,255,0),5 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 5, (0, 0, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 5, (0, 0, 255), -1 )

    cv.imshow('img', img)
    if cv.waitKey(1)== ord('q'):
        break
    
cap.release
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

* Reconocimiento de Personas

** Eigenfaces 

Un Eigenface (en espa√±ol cara propia) es el nombre dado a un conjunto
de vectores propios cuando se utiliza en el problema de visi√≥n
artificial del reconocimiento de rostros humanos. Sirovich y Kirby
desarrollaron el enfoque de usar caras propias para el reconocimiento
y lo usaron Matthew Turk y Alex Pentland en la clasificaci√≥n de
caras. Los vectores propios se derivan de la matriz de covarianza de
la distribuci√≥n de probabilidad sobre el espacio vectorial de alta
dimensi√≥n de im√°genes de rostros. Las caras propias forman un conjunto
base de todas las im√°genes utilizadas para construir la matriz de
covarianza. Esto produce una reducci√≥n de la dimensi√≥n al permitir que
el conjunto m√°s peque√±o de im√°genes base represente las im√°genes de
entrenamiento originales. La clasificaci√≥n se puede lograr comparando
c√≥mo se representan las caras por el conjunto base.

 *Generaci√≥n*
 Se puede generar un conjunto de caras propias mediante la realizaci√≥n
 de un proceso matem√°tico llamado an√°lisis de componentes principales
 (PCA) en un gran conjunto de im√°genes que representan diferentes
 rostros humanos. De manera informal, las caras propias pueden
 considerarse un conjunto de "ingredientes faciales estandarizados",
 derivados del an√°lisis estad√≠stico de muchas im√°genes de
 rostros. Cualquier rostro humano puede considerarse una combinaci√≥n
 de estos rostros est√°ndar. Por ejemplo, la cara de uno podr√≠a estar
 compuesta por la cara promedio m√°s el 10 % de la cara propia 1, el 55
 % de la cara propia 2 e incluso el ‚àí3 % de la cara
 propia 3. Sorprendentemente, no se necesitan muchas caras propias
 combinadas para lograr una aproximaci√≥n justa de la mayor√≠a de las
 caras. Adem√°s, debido a que la cara de una persona no se registra
 mediante una fotograf√≠a digital, sino simplemente como una lista de
 valores (un valor para cada cara propia en la base de datos
 utilizada), se ocupa mucho menos espacio para la cara de cada
 persona.

 Las caras propias que se crean aparecer√°n como √°reas claras y oscuras
 que se organizan en un patr√≥n espec√≠fico. Este patr√≥n es c√≥mo se
 seleccionan las diferentes caracter√≠sticas de una cara para
 evaluarlas y puntuarlas. Habr√° un patr√≥n para evaluar la simetr√≠a, si
 hay alg√∫n estilo de vello facial, d√≥nde est√° la l√≠nea del cabello o
 una evaluaci√≥n del tama√±o de la nariz o la boca. Otras caras propias
 tienen patrones que son menos f√°ciles de identificar, y la imagen de
 la cara propia puede parecerse muy poco a una cara.

 La t√©cnica utilizada en la creaci√≥n de caras propias y su uso para el
 reconocimiento tambi√©n se utiliza fuera del reconocimiento facial:
 reconocimiento de escritura a mano, lectura de labios, reconocimiento
 de voz, lenguaje de se√±as /interpretaci√≥n de gestos con las manos y
 an√°lisis de im√°genes m√©dicas. Por lo tanto, algunos no usan el
 t√©rmino "eigenface", sino que prefieren usar 'eigenimage'.



#+BEGIN_SRC python :results output
import cv2 as cv 
import numpy as np 
import os
dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
print(np.count_nonzero(np.array(labels)==0)) 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloEigenface.xml')

#+END_SRC

#+RESULTS:
: ['lalo']
: 169

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.read('laloEigenface.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        #cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] > 2800:
            cv.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:



** Fisherfaces


 El algoritmo Fisherfaces es una t√©cnica de reconocimiento facial que
 forma parte del campo del aprendizaje autom√°tico y la visi√≥n por
 computadora. Este algoritmo es una extensi√≥n del m√©todo de An√°lisis de
 Componentes Principales (PCA) y fue dise√±ado espec√≠ficamente para
 mejorar la capacidad de reconocimiento en situaciones donde la
 iluminaci√≥n y las expresiones faciales var√≠an significativamente.

 La idea central detr√°s de Fisherfaces es reducir la dimensionalidad de
 las im√°genes faciales manteniendo al mismo tiempo la capacidad de
 distinguir entre diferentes clases (es decir, diferentes
 personas). Esto se logra mediante el An√°lisis Discriminante Lineal
 (LDA), que es la base del m√©todo Fisherfaces. 

 Preprocesamiento: Las im√°genes faciales se normalizan en t√©rminos de
 tama√±o, orientaci√≥n e iluminaci√≥n.

 *An√°lisis de Componentes Principales (PCA)*: Se realiza PCA para reducir
 la dimensionalidad de los datos. PCA identifica las direcciones en las
 que los datos var√≠an m√°s y proyecta los datos en un espacio de menor
 dimensi√≥n preservando estas variaciones principales.

 *An√°lisis Discriminante Lineal (LDA)*: Despu√©s de aplicar PCA, se
 utiliza LDA para encontrar las combinaciones lineales de
 caracter√≠sticas que mejor separan las diferentes clases (diferentes
 personas). Mientras que PCA busca direcciones que maximizan la
 varianza en los datos, LDA busca maximizar la separaci√≥n entre las
 diferentes clases.

 *Proyecci√≥n y Clasificaci√≥n*: Las im√°genes se proyectan en el espacio de
 caracter√≠sticas obtenido por PCA y LDA. Luego, se utiliza un
 clasificador (como k-NN o m√°quinas de vectores de soporte) para
 identificar a qu√© clase (persona) pertenece cada imagen proyectada
 bas√°ndose en las caracter√≠sticas extra√≠das.

 El algoritmo Fisherfaces es particularmente efectivo en situaciones
 donde las variaciones entre las im√°genes de una misma clase (por
 ejemplo, las diferentes expresiones faciales de una persona) son
 menores en comparaci√≥n con las variaciones entre clases diferentes
 (diferentes personas). Esto lo hace robusto frente a cambios en la
 iluminaci√≥n y las expresiones faciales, siendo una t√©cnica popular en
 aplicaciones de reconocimiento facial.

#+CAPTION: Script para leer un dataset y generar el entrenamiento con FisherFaces
 #+BEGIN_SRC python ::results
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloFisherFace.xml')


 #+END_SRC

 #+RESULTS:
 : None

 #+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.read('laloFisherFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 500:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()



 #+END_SRC

 #+RESULTS:
 : None



** LBPH
El LBPH es un enfoque simple y efectivo para el reconocimiento
facial. A diferencia de otros m√©todos que operan en todo el rostro, el
LBPH trabaja examinando caracter√≠sticas locales. Su popularidad se
debe a su simplicidad, velocidad y buen rendimiento, incluso en
condiciones de iluminaci√≥n desafiantes. Aqu√≠ est√° c√≥mo funciona:

Divisi√≥n de la Imagen en Celdas: La imagen del rostro se divide en
peque√±as regiones o celdas.

*Calculo de Patrones Binarios Locales (LBP):* Para cada p√≠xel en una
regi√≥n, se compara su intensidad con las de sus vecinos (generalmente
8 vecinos circundantes). Si la intensidad del vecino es mayor o igual
que el p√≠xel central, se asigna un 1, de lo contrario un 0. Esto
genera un n√∫mero binario de 8 d√≠gitos (o un n√∫mero decimal despu√©s de
la conversi√≥n) para cada p√≠xel.

*Histogramas:* Se calcula un histograma de estas etiquetas LBP para cada
celda. Los histogramas cuentan la frecuencia de cada n√∫mero obtenido
en el paso anterior dentro de la celda.

*Concatenaci√≥n de Histogramas:* Los histogramas de todas las celdas se
concatenan en un solo vector de caracter√≠sticas. Este vector describe
las caracter√≠sticas locales de la imagen de la cara.

*Reconocimiento:* Para reconocer un rostro desconocido, se calcula su
vector de caracter√≠sticas LBPH y se compara con los vectores de
caracter√≠sticas de las caras conocidas (generalmente usando una medida
de distancia, como la distancia euclidiana). La imagen desconocida se
identifica como la clase (es decir, la persona) cuyo vector de
caracter√≠sticas conocido sea m√°s cercano al del rostro desconocido.

El LBPH es eficaz en diversas condiciones y no requiere un
preprocesamiento tan intenso como otros m√©todos de reconocimiento
facial. Puede manejar variaciones en iluminaci√≥n y expresi√≥n facial
bastante bien. Adem√°s, su implementaci√≥n es relativamente sencilla, lo
que lo hace popular para aplicaciones en tiempo real y sistemas
embebidos.

#+BEGIN_SRC python
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloLBPHFace.xml')

#+END_SRC

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.read('laloLBPHFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 70:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2) 
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()


#+END_SRC


* Redes Neuronales Convolucionales 

** ¬øC√≥mo funcionan las Convolutional Neural Networks?


La Red Neuronal Convolucional (CNN) es una forma avanzada de Red
Neuronal Artificial dise√±ada para el procesamiento de im√°genes. Emula
la manera en que el cortex visual humano procesa la informaci√≥n
visual, lo que le permite identificar y clasificar distintas
caracter√≠sticas en las im√°genes. Esta capacidad la convierte en una
herramienta eficaz para la "visi√≥n" computarizada y el reconocimiento
de objetos.

Las CNN est√°n compuestas por m√∫ltiples capas ocultas, cada una
especializada y jerarquizada. Las capas iniciales suelen detectar
elementos simples como l√≠neas y curvas. A medida que la informaci√≥n
avanza a trav√©s de la red, las capas subsiguientes procesan aspectos
m√°s complejos, como formas espec√≠ficas y patrones. Esta progresi√≥n
permite a las capas m√°s profundas reconocer objetos m√°s complejos,
como rostros o siluetas animales.

Para que una CNN aprenda a identificar una amplia gama de objetos en
im√°genes, es crucial entrenarla con un extenso conjunto de datos. Por
ejemplo, para el reconocimiento de gatos, se necesitar√≠an miles de
im√°genes que muestren variaciones en color, raza, postura y
entorno. Este entrenamiento supervisado permite a la red aprender las
caracter√≠sticas distintivas de cada objeto y generalizar esta
comprensi√≥n para reconocer nuevas im√°genes de manera efectiva. As√≠,
una CNN bien entrenada puede distinguir un gato independientemente de
su posici√≥n, color o tama√±o, adapt√°ndose a una amplia gama de
situaciones visuales.

** Convoluci√≥n

La convoluci√≥n es una operaci√≥n matem√°tica que combina dos conjuntos
de informaci√≥n. En el contexto de redes neuronales convolucionales
(CNN), la convoluci√≥n se refiere a aplicar un filtro (tambi√©n conocido
como kernel) a una imagen de entrada para producir un mapa de
caracter√≠sticas.  C√≥mo Funciona:

Un filtro de tama√±o fijo recorre la imagen de entrada.  En cada
posici√≥n del filtro, se realiza una multiplicaci√≥n punto a punto entre
los valores del filtro y los valores de la imagen bajo el filtro.  Los
productos resultantes se suman para obtener un √∫nico valor en el mapa
de caracter√≠sticas.  Este proceso se repite para cada posici√≥n del
filtro, generando un mapa de caracter√≠sticas que resalta ciertas
caracter√≠sticas de la imagen, como bordes, texturas y patrones.
Prop√≥sitos:

Extraer caracter√≠sticas locales de la imagen.  Capturar patrones
espaciales y relaciones en la imagen.  Reducir la dimensionalidad
espacial de la imagen mientras se mantiene la informaci√≥n relevante.


** Padding
A√±adir p√≠xeles adicionales alrededor de los bordes de la entrada antes
de la operaci√≥n de convoluci√≥n para controlar el tama√±o de la salida y
conservar informaci√≥n de los bordes.

** Strides (Paso)

N√∫mero de p√≠xeles que el filtro (kernel) se desplaza sobre la entrada
durante la operaci√≥n de convoluci√≥n. Un stride mayor que 1 reduce la
resoluci√≥n espacial de la salida.

** Max Pooling Definici√≥n:

Operaci√≥n que reduce la dimensionalidad espacial de la entrada tomando
el valor m√°ximo dentro de una ventana espec√≠fica, manteniendo las
caracter√≠sticas m√°s importantes y reduciendo el tama√±o de la salida.

** Stacking (Apilamiento)

Pr√°ctica de a√±adir m√∫ltiples capas convolucionales y de pooling en una
secuencia dentro de una red neuronal, permitiendo la construcci√≥n de
redes profundas y la extracci√≥n de caracter√≠sticas cada vez m√°s
complejas.

#+startup: inlineimages
#+ATTR_LATEX: :width 0.3\textwidth
[[file:img/conv.png]]


** YOLO
YOLO (You Only Look Once) es un algoritmo de detecci√≥n de objetos en
tiempo real que se ha destacado por su velocidad y precisi√≥n. Fue
desarrollado por Joseph Redmon y  colaboradores. 

***  YOLO (You Only Look Once)

- *Enfoque Unificado*: A diferencia de otros m√©todos que aplican el
  proceso de detecci√≥n en varias etapas, YOLO trata la detecci√≥n de
  objetos como un problema de regresi√≥n √∫nico, convirtiendo la imagen
  de entrada directamente en coordenadas de cajas delimitadoras y
  probabilidades de clase en una sola pasada de la red neuronal.

- *Red Convolucional*: YOLO utiliza una red neuronal convolucional
  profunda para extraer caracter√≠sticas de la imagen.

- *Divisi√≥n de la Imagen*: La imagen de entrada se divide en una
  cuadr√≠cula \textit{times}. Cada celda de la cuadr√≠cula es
  responsable de predecir un n√∫mero fijo de cajas delimitadoras y las
  probabilidades de las clases para esas cajas.

***  Predicciones
- *Cajas Delimitadoras*: Cada celda de la cuadr√≠cula predice un conjunto
  de cajas delimitadoras (bounding boxes), cada una con:
  - Coordenadas \( (x, y) \): La posici√≥n del centro de la caja con respecto a los bordes de la celda de la cuadr√≠cula.
  - Ancho y alto \( (w, h) \): Dimensiones de la caja delimitadora relativas al tama√±o de la imagen.
  - Confianza de la caja: Una puntuaci√≥n que indica la probabilidad de que la caja contenga un objeto y la precisi√≥n de la caja delimitadora.
- *Probabilidades de Clase*: Cada celda tambi√©n predice la probabilidad de las clases para los objetos presentes en las cajas delimitadoras.

***  Funcionamiento
- *Paso 1: Entrada*: Una imagen de entrada se pasa a trav√©s de la red
  convolucional.
- *Paso 2: Divisi√≥n de Cuadr√≠cula*: La imagen se divide en una
  cuadr√≠cula  $\times $.
- *Paso 3: Predicciones de la Red*: La red predice $( B )$ cajas
  delimitadoras y $ C $ probabilidades de clase para cada celda de
  la cuadr√≠cula.
- *Paso 4: Filtrado de Cajas*: Se aplican t√©cnicas como el umbral de
  confianza y la supresi√≥n de no-m√°ximos (Non-Maximum Suppression,
  NMS) para filtrar las cajas delimitadoras y eliminar las
  redundantes.
- *Paso 5: Salida*: Las cajas delimitadoras finales con las
  probabilidades de clase correspondientes se devuelven como
  resultado.

*** Ventajas
- *Velocidad*: YOLO es extremadamente r√°pido porque procesa la imagen
  completa en una sola pasada de la red, lo que permite la detecci√≥n
  en tiempo real.
- *Contexto Global*: Al considerar la imagen completa, YOLO puede
  capturar el contexto global, lo que reduce los errores de detecci√≥n
  en comparaci√≥n con los enfoques que solo consideran regiones
  locales.
- *Simpleza*: El enfoque unificado simplifica el proceso de detecci√≥n y
  lo hace m√°s f√°cil de implementar y entrenar.

*** Limitaciones
- *Detecci√≥n de Objetos Peque√±os*: Puede tener dificultades para
  detectar objetos muy peque√±os debido a la divisi√≥n de la imagen en
  una cuadr√≠cula de tama√±o fijo.
- *Precisi√≥n*: Aunque es muy r√°pida, la precisi√≥n de YOLO puede ser
  inferior en comparaci√≥n con m√©todos m√°s complejos como Faster R-CNN
  en ciertos escenarios.
- *Trade-off Velocidad-Precisi√≥n*: Existe un balance entre la velocidad
  y la precisi√≥n; versiones m√°s r√°pidas de YOLO pueden sacrificar algo
  de precisi√≥n por un aumento en la velocidad.

*** Evoluci√≥n
- *YOLOv2, YOLOv3, YOLOv4, y YOLOv5*: Cada nueva versi√≥n de YOLO ha
  introducido mejoras en la arquitectura y el entrenamiento,
  aumentando la precisi√≥n y manteniendo o mejorando la velocidad.
- *YOLOv4 y YOLOv5*: Incorporan t√©cnicas avanzadas como mecanismos de
  atenci√≥n, mejores m√©todos de anclaje de cajas, y t√©cnicas de
  entrenamiento m√°s robustas, resultando en mejoras significativas en
  la detecci√≥n de objetos.

YOLO es un algoritmo de detecci√≥n de objetos eficiente y r√°pido que
utiliza un enfoque unificado para predecir cajas delimitadoras y
probabilidades de clase en una sola pasada de la red. Es especialmente
√∫til para aplicaciones en tiempo real donde la velocidad es cr√≠tica.

** Ejemplo YOLO

#+BEGIN_SRC python
import cv2
import numpy as np

# Rutas de los archivos de configuraci√≥n y pesos del modelo YOLOv3
config = "/home/likcos/yolo/yolov3.cfg"
weights = "/home/likcos/yolo/yolov3.weights"
# Cargar los nombres de las etiquetas desde el archivo coco.names
LABELS = open("/home/likcos/yolo/coco.names").read().split("\n")
# Generar colores aleatorios para cada etiqueta
colors = np.random.randint(0, 255, size=(len(LABELS), 3), dtype="uint8")

# Cargar la red YOLO desde los archivos de configuraci√≥n y pesos
net = cv2.dnn.readNetFromDarknet(config, weights)

# Leer la imagen desde el archivo especificado
image = cv2.imread("/home/likcos/yolo/horse.jpg")
height, width, _ = image.shape

# Crear un blob a partir de la imagen (preprocesamiento)
blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)

# Obtener los nombres de las capas de la red
ln = net.getLayerNames()

# Obtener solo los nombres de las capas de salida
ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]

# Configurar la entrada de la red con el blob
net.setInput(blob)
# Realizar la pasada hacia adelante (forward pass) y obtener las salidas
outputs = net.forward(ln)

# Inicializar listas para las cajas delimitadoras, confidencias y IDs de clase
boxes = []
confidences = []
classIDs = []

# Procesar cada una de las salidas de la red
for output in outputs:
    for detection in output:
        # Obtener las puntuaciones de todas las clases
        scores = detection[5:]
        # Obtener el ID de la clase con la mayor puntuaci√≥n
        classID = np.argmax(scores)
        # Obtener la confianza (probabilidad) de la clase seleccionada
        confidence = scores[classID]

        # Filtrar detecciones con una confianza baja
        if confidence > 0.5:
            # Escalar las coordenadas de la caja delimitadora a las dimensiones de la imagen original
            box = detection[:4] * np.array([width, height, width, height])
            (x_center, y_center, w, h) = box.astype("int")
            x = int(x_center - (w / 2))
            y = int(y_center - (h / 2))

            # Agregar la caja, confianza e ID de clase a las listas respectivas
            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            classIDs.append(classID)

# Aplicar la supresi√≥n de no-m√°ximos para eliminar las cajas redundantes
idx = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.5)
print("idx:", idx)

# Dibujar las cajas finales en la imagen
if len(idx) > 0:
    for i in idx:
        (x, y) = (boxes[i][0], boxes[i][1])
        (w, h) = (boxes[i][2], boxes[i][3])

        # Seleccionar un color para la clase detectada
        color = colors[classIDs[i]].tolist()
        # Crear el texto con la etiqueta y la confianza
        text = "{}: {:.3f}".format(LABELS[classIDs[i]], confidences[i])
        # Dibujar la caja delimitadora y poner el texto en la imagen
        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# Mostrar la imagen resultante
cv2.imshow("Image", image)
cv2.waitKey(0)
cv2.destroyAllWindows()

#+END_SRC

#+RESULTS:

** MobileNet SSD

MobileNet SSD (Single Shot MultiBox Detector) es una arquitectura de
red neuronal profunda utilizada para la detecci√≥n de objetos en tiempo
real. Combina la eficiencia y velocidad de MobileNet con la precisi√≥n
y versatilidad de SSD, lo que la hace ideal para aplicaciones m√≥viles
y embebidas. Aqu√≠ tienes una descripci√≥n m√°s detallada:

***  MobileNet
- *Arquitectura*: MobileNet es una red neuronal convolucional profunda
  que se caracteriza por su eficiencia computacional y tama√±o
  reducido. Utiliza convoluciones separables en profundidad (depthwise
  separable convolutions), lo que reduce significativamente el n√∫mero
  de par√°metros y operaciones de c√°lculo.
- *Caracter√≠sticas*:
  - *Convoluciones separables en profundidad*: Descompone la convoluci√≥n
    est√°ndar en dos operaciones m√°s simples y menos costosas: una
    convoluci√≥n en profundidad (depthwise convolution) y una
    convoluci√≥n puntual (pointwise convolution).
  - *Bloques de construcci√≥n*: Cada bloque consta de una convoluci√≥n en
    profundidad seguida de una convoluci√≥n puntual y una funci√≥n de
    activaci√≥n no lineal.
  - *Tama√±o reducido*: Dise√±ada para ser eficiente en dispositivos con
    recursos limitados, como tel√©fonos m√≥viles y dispositivos IoT.

***  SSD (Single Shot MultiBox Detector)
- *Arquitectura*: SSD es un algoritmo de detecci√≥n de objetos que
  predice m√∫ltiples cajas delimitadoras (bounding boxes) y sus
  probabilidades de clase en una sola pasada a trav√©s de la red (de
  ah√≠ el t√©rmino "Single Shot").
- *Caracter√≠sticas*:
  - *Predicciones m√∫ltiples*: Realiza predicciones en m√∫ltiples escalas
    desde varias capas de la red, lo que permite detectar objetos de
    diferentes tama√±os.
  - *Mapas de caracter√≠sticas*: Utiliza mapas de caracter√≠sticas de
    diferentes niveles de resoluci√≥n para detectar objetos de
    diferentes tama√±os.
  - *Cajas predeterminadas*: Utiliza un conjunto de cajas
    predeterminadas (default boxes) con diferentes proporciones y
    escalas en cada mapa de caracter√≠sticas.

***  MobileNet SSD
- *Combina lo mejor de ambos mundos*: Integra la eficiencia y velocidad
  de MobileNet con la precisi√≥n y capacidad de detecci√≥n en tiempo
  real de SSD.
- *Aplicaciones*:
  - *Dispositivos m√≥viles*: Ideal para aplicaciones en tel√©fonos m√≥viles
    y tablets debido a su bajo requerimiento computacional.
  - *IoT y sistemas embebidos*: Perfecta para sistemas embebidos y
    dispositivos IoT donde los recursos de procesamiento son
    limitados.
  - *Drones y rob√≥tica*: Utilizada en drones y robots que requieren
    capacidades de detecci√≥n en tiempo real.

***  Beneficios
- *Eficiencia computacional*: Bajo consumo de energ√≠a y recursos,
  permitiendo su implementaci√≥n en dispositivos con limitaciones de
  hardware.
- *Detecci√≥n en tiempo real*: Capacidad de procesar y detectar objetos
  r√°pidamente, lo que es crucial para aplicaciones en tiempo real.
- *Versatilidad*: Puede ser entrenada para detectar una amplia variedad
  de objetos en diferentes entornos y condiciones.

***  Limitaciones
- *Precisi√≥n vs. otros modelos*: Aunque es eficiente, su precisi√≥n puede
  ser menor en comparaci√≥n con otros modelos m√°s complejos y pesados.
- *Detecci√≥n de objetos peque√±os*: Puede tener dificultades para
  detectar objetos muy peque√±os debido a la reducci√≥n de resoluci√≥n en
  capas profundas.

MobileNet SSD es una arquitectura  eficiente para la
detecci√≥n de objetos en tiempo real, especialmente adecuada para
dispositivos con recursos limitados.

#+BEGIN_SRC python
import cv2

# Rutas de los archivos de configuraci√≥n y modelo de MobileNet SSD
prototxt = "/home/likcos/yolo/MobileNetSSD_deploy.prototxt.txt"
model = "/home/likcos/yolo/MobileNetSSD_deploy.caffemodel"

# Diccionario de clases con sus respectivos IDs
classes = {0: "background", 1: "aeroplane", 2: "bicycle",
           3: "bird", 4: "boat",
           5: "bottle", 6: "bus",
           7: "car", 8: "cat",
           9: "chair", 10: "cow",
           11: "diningtable", 12: "dog",
           13: "horse", 14: "motorbike",
           15: "person", 16: "pottedplant",
           17: "sheep", 18: "sofa",
           19: "train", 20: "tvmonitor"}

# Cargar la red MobileNet SSD desde los archivos de configuraci√≥n y modelo
net = cv2.dnn.readNetFromCaffe(prototxt, model)

# Leer la imagen desde el archivo especificado
image = cv2.imread("/home/likcos/yolo/paj1.png")
height, width, _ = image.shape
# Redimensionar la imagen a 300x300 p√≠xeles, tama√±o de entrada esperado por la red
image_resized = cv2.resize(image, (300, 300))

# Crear un blob a partir de la imagen redimensionada (preprocesamiento)
blob = cv2.dnn.blobFromImage(image_resized, 0.007843, (300, 300), (127.5, 127.5, 127.5))
print("blob.shape:", blob.shape)

# Configurar la entrada de la red con el blob
net.setInput(blob)
# Realizar la pasada hacia adelante (forward pass) y obtener las detecciones
detections = net.forward()

# Iterar sobre cada detecci√≥n
for detection in detections[0][0]:
    print(detection)

    # Filtrar detecciones con una confianza mayor al 45%
    if detection[2] > 0.45:
        # Obtener la etiqueta de la clase
        label = classes[detection[1]]
        print("Label:", label)
        # Escalar las coordenadas de la caja delimitadora a las dimensiones de la imagen original
        box = detection[3:7] * [width, height, width, height]
        x_start, y_start, x_end, y_end = int(box[0]), int(box[1]), int(box[2]), int(box[3])

        # Dibujar la caja delimitadora y las etiquetas en la imagen
        cv2.rectangle(image, (x_start, y_start), (x_end, y_end), (0, 255, 0), 2)
        cv2.putText(image, "Conf: {:.2f}".format(detection[2] * 100), (x_start, y_start - 5), 1, 1.2, (255, 0, 0), 2)
        cv2.putText(image, label, (x_start, y_start - 25), 1, 1.2, (255, 0, 0), 2)

# Mostrar la imagen resultante con las detecciones
cv2.imshow("Image", image)
cv2.waitKey(0)
cv2.destroyAllWindows()


#+END_SRC


* Desaf√≠os y Consideraciones √âticas
  - Desaf√≠os T√©cnicos: Precisi√≥n, grandes conjuntos de datos, computaci√≥n intensiva.
  - Cuestiones de Privacidad: Preocupaciones sobre reconocimiento facial y vigilancia.
  - Futuro de la Visi√≥n por Computadora: Impacto en la sociedad y desarrollo continuo.




* Conclusi√≥n y Futuro de la Visi√≥n por Computadora
  - Resumen: Repaso de los puntos clave.
  - Futuras Tendencias: Inteligencia artificial, aprendizaje profundo.
  - Preguntas y Discusi√≥n: Invitaci√≥n a participar.


